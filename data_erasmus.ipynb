{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n##  Pipeline Executive Summary\n\n### Complete Transformation\n\n| Metric | Before | After | Change |\n|---------|-------|---------|--------|\n| **Total records** | 5,955,075 | 2,082,071 | HE 2017-2024 filtered |\n| **Duplicates** | 9,165 (0.15%) | 0 | Removed |\n| **Source files** | 11 CSVs (2014-2024) | 1 unified dataset | Consolidated |\n| **Columns** | 19-21 (inconsistent) | 26 (standardized) | Normalized |\n| **Date format** | Mixed | ISO 8601 | Standardized |\n| **Country names** | Multiple variants | ISO2 + canonical name | Unified |\n\n### Data Quality\n\n- **100% of countries normalized** by ISO2 code\n- **0 exact duplicates**\n- **621,084 age outliers** converted to NA (impossible outliers <10 or >65)\n- **16,069 durations = 0** converted to NA\n- **Null tokens** unified (\"unknown\", \"n/a\", \"???\", etc. -> NA)\n\n### Final Dataset for Power BI\n\n- **Focus**: Higher Education (ISCED 6-8)\n- **Period**: 2017-2024 (mobility start years)\n- **Variables**: 26 analytical columns\n- **Formats**: Parquet (compressed) + CSV (universal)\n\n---\n\nFor technical details of the cleaning process, see [`PIPELINE.md`](docs/en/PIPELINE.md)"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries for the process\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-HYIXqgm7fbg",
    "outputId": "f0c34fb1-96f6-4686-8532-a340fccdc02f"
   },
   "outputs": [],
   "source": [
    "# - Check if the annual CSVs (2014–2024) have the same column structure.\n",
    "# - Detect differences between years before performing the full load.\n",
    "\n",
    "# - Normalize column names (lowercase + underscores) for consistent comparison.\n",
    "folder_path = \"bases_datos_erasmus\"\n",
    "years = range(2014, 2025)  # includes 2024\n",
    "\n",
    "\n",
    "def normalize_columns(columns):\n",
    "    \"\"\"\n",
    "    Normalize column names to compare between years.\n",
    "    In this block I do a basic normalization (lowercase and underscores).\n",
    "    \"\"\"\n",
    "    normalized = []\n",
    "    for col in columns:\n",
    "        col = col.strip().lower()\n",
    "        col = re.sub(r\"\\s+\", \" \", col)      \n",
    "        col = col.replace(\" \", \"_\")         \n",
    "        normalized.append(col)\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def read_header_with_fallback(file_path, sep=\";\"):\n",
    "    \"\"\"\n",
    "    Read ONLY the CSV header (nrows=0) trying various encodings.\n",
    "    Returns (df_header, encoding_used).\n",
    "    \"\"\"\n",
    "    encodings_to_try = [\"utf-8\", \"utf-8-sig\", \"latin1\"]\n",
    "    last_error = None\n",
    "\n",
    "    for enc in encodings_to_try:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep=sep, nrows=0, encoding=enc)\n",
    "            return df, enc\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "\n",
    "    raise last_error\n",
    "\n",
    "\n",
    "column_sets = {}   # columns (normalized) by year\n",
    "errors = {}        # read errors by year\n",
    "\n",
    "print(\"Verifying file structure...\")\n",
    "\n",
    "for year in years:\n",
    "    file_name = f\"Erasmus-KA1-Mobility-Data-{year}.csv\"\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    try:\n",
    "        df_header, used_enc = read_header_with_fallback(file_path, sep=\";\")\n",
    "        column_sets[year] = normalize_columns(df_header.columns)\n",
    "        print(f\"  {year}: {len(df_header.columns)} columns (encoding={used_enc})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        errors[year] = str(e)\n",
    "        print(f\"  {year}: Error reading {file_name}: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\nComparing columns across years...\")\n",
    "\n",
    "if not column_sets:\n",
    "    print(\"Could not read any file. Check paths, names and separator.\")\n",
    "else:\n",
    "    # Use the first year that was read correctly as reference\n",
    "    base_year = min(column_sets.keys())\n",
    "    base_cols = set(column_sets[base_year])\n",
    "\n",
    "    for year in sorted(column_sets.keys()):\n",
    "        if year == base_year:\n",
    "            continue\n",
    "\n",
    "        current_cols = set(column_sets[year])\n",
    "\n",
    "        if current_cols != base_cols:\n",
    "            print(f\"\\n  Differences detected in {year} (base={base_year}):\")\n",
    "            print(\"    - Missing:\", base_cols - current_cols)\n",
    "            print(\"    - New:\", current_cols - base_cols)\n",
    "        else:\n",
    "            print(f\"  {year} has the same columns as {base_year}.\")\n",
    "\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(\"  Years OK:\", sorted(column_sets.keys()))\n",
    "if errors:\n",
    "    print(\"  Years with error:\", sorted(errors.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DQA0zaJIB2pv",
    "outputId": "d0e61516-0a45-471e-aef3-b0a260a1aba9"
   },
   "outputs": [],
   "source": "# In the previous block I verified that small differences exist between years in column names.\n# In this block I now load the complete CSVs and apply a unification strategy to be able to concatenate them:\n# - Normalize column names (lowercase, underscores, etc.)\n# - Rename equivalent columns to canonical names (RENAME_MAP)\n# - Remove residual \"Unnamed\" type columns\n# - Add source_file_year for traceability\n# - Align all columns between years and concatenate into df_total\n\n\nfolder_path = \"bases_datos_erasmus\"\nyears = range(2014, 2025)\n\ndef normalize_columns(cols):\n    \"\"\"\n    Normalize column names so all years follow the same format.\n    \"\"\"\n    out = []\n    for c in cols:\n        c = str(c).strip().lower()\n        c = re.sub(r\"\\s+\", \" \", c)\n        c = c.replace(\" \", \"_\")\n        c = c.replace(\"/\", \"_\")\n        c = re.sub(r\"[()]\", \"\", c)\n        c = re.sub(r\"__+\", \"_\", c)\n        out.append(c)\n    return out\n\n# Mapping to canonical names (equivalent columns between years)\nRENAME_MAP = {\n    \"sending_organisation\": \"sending_organization\",\n    \"receiving_organisation\": \"receiving_organization\",\n    \"mobility_duration_-_calendar_days\": \"mobility_duration\",\n    \"mobility_duration_in_days\": \"mobility_duration\",\n    \"mobility_start_year_month\": \"mobility_start_month\",\n    \"actual_participants_contracted_projects\": \"actual_participants\",\n}\n\ndef read_csv_with_fallback(path, sep=\";\"):\n    \"\"\"\n    Read the CSV trying different encodings to avoid read errors.\n    Returns (df, encoding_used).\n    \"\"\"\n    encodings = [\"utf-8\", \"utf-8-sig\", \"latin1\"]\n    last_error = None\n    for enc in encodings:\n        try:\n            df = pd.read_csv(path, sep=sep, low_memory=False, encoding=enc)\n            return df, enc\n        except Exception as e:\n            last_error = e\n    raise last_error\n\ndf_list = []\nall_cols = set()\n\nprint(\"Loading and unifying files by year...\")\n\nfor year in years:\n    file_name = f\"Erasmus-KA1-Mobility-Data-{year}.csv\"\n    file_path = os.path.join(folder_path, file_name)\n\n    try:\n        # 1) Load the CSV for the year\n        df, used_enc = read_csv_with_fallback(file_path, sep=\";\")\n\n        # 2) Normalize column names and apply canonical renaming\n        df.columns = normalize_columns(df.columns)\n        df = df.rename(columns=RENAME_MAP)\n\n        # 3) Remove residual \"Unnamed\" type columns\n        unnamed_cols = [c for c in df.columns if c.startswith(\"unnamed\")]\n        if unnamed_cols:\n            df = df.drop(columns=unnamed_cols)\n\n        # 4) Add traceability of source year\n        df[\"source_file_year\"] = year\n\n        # 5) Accumulate DF and columns to align at the end\n        df_list.append(df)\n        all_cols |= set(df.columns)\n\n        print(f\" {year} loaded: {df.shape[0]:,} rows, {df.shape[1]} columns (encoding={used_enc})\")\n\n    except Exception as e:\n        print(f\" Error in {year}: {e}\")\n\nif not df_list:\n    raise ValueError(\"Could not load any file. Check path, names and separator.\")\n\n# Fixed column order so the final dataset is stable\nall_cols = sorted(all_cols)\n\n# Align all years to the same set of columns\ndf_list_aligned = [d.reindex(columns=all_cols) for d in df_list]\n\n# Concatenate everything into df_total\ndf_total = pd.concat(df_list_aligned, ignore_index=True)\n\nprint(f\"\\n{'='*60}\\nFILES MERGED\\n{'='*60}\")\nprint(f\"Total records: {df_total.shape[0]:,}\")\nprint(f\"Total columns:  {df_total.shape[1]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before normalizing, I review what distinct values appear in the \"academic_year\" column.\n",
    "# This helps me detect different formats (for example: \"2020-2021\" vs \"2020-21\").\n",
    "print(df_total[\"academic_year\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to leave all values of \"academic_year\" with the same format.\n",
    "# Goal: for it to always remain as \"YYYY-YY\" (for example, \"2020-21\").\n",
    "def norm_academic_year(x):\n",
    "    # If the value is null (NaN), I keep it as null\n",
    "    if pd.isna(x):\n",
    "        return pd.NA\n",
    "\n",
    "    # Convert to string and remove spaces in case values come like \" 2020-2021 \"\n",
    "    s = str(x).strip()\n",
    "\n",
    "    # Case 1: full format \"YYYY-YYYY\" (e.g. \"2020-2021\")\n",
    "    # In this case, I convert it to \"YYYY-YY\" (e.g. \"2020-21\")\n",
    "    m = re.fullmatch(r\"(\\d{4})-(\\d{4})\", s)\n",
    "    if m:\n",
    "        return f\"{m.group(1)}-{m.group(2)[-2:]}\"\n",
    "\n",
    "    # Case 2: already reduced format \"YYYY-YY\" (e.g. \"2020-21\")\n",
    "    # If it's already correct, I leave it as is\n",
    "    m = re.fullmatch(r\"(\\d{4})-(\\d{2})\", s)\n",
    "    if m:\n",
    "        return s\n",
    "\n",
    "    # If a strange format appears that doesn't match the previous ones,\n",
    "    # I return it the same to review it later.\n",
    "    return s\n",
    "\n",
    "\n",
    "# Apply normalization and overwrite the column with unified values\n",
    "df_total[\"academic_year\"] = df_total[\"academic_year\"].apply(norm_academic_year)\n",
    "\n",
    "# Quick check: review unique values again to make sure it remained homogeneous\n",
    "print(df_total[\"academic_year\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# I use this block to review duplicates in the final dataset.\n# I distinguish two cases:\n# 1) Exact duplicates: identical rows in all columns (including academic_year).\n# 2) Repeated rows ignoring academic_year: same characteristics but in different academic years,\n#    which could happen if a record repeats between years or if there is some form of \"overlap\".\ncol_year = \"academic_year\"\n\n# Columns I show as example when I find duplicates\n# (just for quick inspection and so the output is not huge)\ncols_show = [\"academic_year\", \"sending_country\", \"receiving_city\", \"participant_age\", \"mobility_duration\"]\n\n\n# Assign df_total to a variable for convenience.\n# I don't use copy() to avoid wasting extra memory, since I don't modify the DataFrame here.\ndf = df_total\n\n\n# 1) Exact duplicates (all columns equal)\nn = len(df)\nn_dup = df.duplicated().sum()\nprint(f\"Exact duplicates: {n_dup:,} ({n_dup / n * 100:.4f}%)\")\n\n# If they exist, I show an example of some rows to see the pattern\nif n_dup:\n    print(\"\\nExample exact duplicates (5 rows):\")\n    print(df.loc[df.duplicated(keep=False), cols_show].head(5))\n\n\n# 2) Repeated rows ignoring academic_year\n# First I convert academic_year to a numeric year (I keep the first year: \"2015-16\" -> 2015)\n# This is just to analyze if the same \"group\" appears in more than one year.\nyear_temp = pd.to_numeric(df[col_year].astype(str).str[:4], errors=\"coerce\").astype(\"Int64\")\n\n# Create a list with all columns except academic_year to compare \"everything except the year\"\ncols_compare = [c for c in df.columns if c != col_year]\n\n# To efficiently detect repeats, I create a hash per row using those columns\nh = pd.util.hash_pandas_object(df[cols_compare], index=False)\n\n# Mark as repeated the rows whose hash appears more than once\nmask_rep = h.duplicated(keep=False)\nprint(f\"\\nRepeated rows ignoring '{col_year}': {mask_rep.sum():,}\")\n\n# Now I look at how many of those groups appear in more than one year\ntmp = pd.DataFrame({\"h\": h[mask_rep], \"y\": year_temp[mask_rep]})\nmulti = tmp.groupby(\"h\")[\"y\"].nunique()\n\n# Count how many hashes have more than one associated year (multi-year)\nn_multi_groups = (multi > 1).sum()\nprint(f\"Groups repeated in multiple years: {n_multi_groups:,}\")\n\n# If there is any multi-year case, I show an example to better understand it\nif n_multi_groups:\n    ex_hash = multi[multi > 1].index[0]\n    print(\"\\nExample multi-year (5 rows):\")\n    print(df.loc[h == ex_hash, cols_show].head(5))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the previous analysis I saw that there were exact duplicates (identical rows in all columns).\n",
    "# Since they are 100% repeated records, I decide to remove them to avoid counting the same observation twice.\n",
    "# I keep the first occurrence of each row and remove the rest.\n",
    "\n",
    "n_before = len(df_total)\n",
    "n_dup = df_total.duplicated().sum()\n",
    "\n",
    "print(f\"Records before: {n_before:,}\")\n",
    "print(f\"Exact duplicates detected: {n_dup:,} ({n_dup / n_before * 100:.4f}%)\")\n",
    "\n",
    "# Remove exact duplicates and reset the index to leave the DataFrame clean\n",
    "df_total = df_total.drop_duplicates(keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "n_after = len(df_total)\n",
    "print(f\"Records after: {n_after:,}\")\n",
    "print(f\"Removed: {n_before - n_after:,}\")\n",
    "\n",
    "# Final check: confirm that there are no exact duplicates left\n",
    "print(\"Remaining duplicates:\", df_total.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I review the most frequent values of \"mobility_start_month\" to understand\n",
    "# why the column is of type object (usually happens due to mixed formats,\n",
    "# strange values, or because it comes as text \"YYYY-MM\").\n",
    "df_total[\"mobility_start_month\"].value_counts(dropna=False).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't modify the original column to be able to go back if needed.\n",
    "# Create an auxiliary column in date format (\"mobility_start_ym\") from the text.\n",
    "# Use errors=\"coerce\" so values that don't match the format become NaT (date null).\n",
    "df_total[\"mobility_start_ym\"] = pd.to_datetime(\n",
    "    df_total[\"mobility_start_month\"].astype(str),\n",
    "    format=\"%Y-%m\",\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# From the date, I extract the year and month as separate columns.\n",
    "# Use Int64 (nullable) to allow nulls without converting to float.\n",
    "df_total[\"mobility_start_year\"] = df_total[\"mobility_start_ym\"].dt.year.astype(\"Int64\")\n",
    "df_total[\"mobility_start_month_num\"] = df_total[\"mobility_start_ym\"].dt.month.astype(\"Int64\")\n",
    "\n",
    "# Quick checks:\n",
    "# 1) How many values could not be converted (remained as NaT)\n",
    "print(\"Nulls mobility_start_ym:\", df_total[\"mobility_start_ym\"].isna().sum())\n",
    "\n",
    "# 2) Distribution of months (including nulls) to see if it makes sense\n",
    "print(df_total[\"mobility_start_month_num\"].value_counts(dropna=False).sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Quick check: how many nulls there are and if there are values with decimals\n",
    "# In theory \"actual_participants\" should be an integer number and > 0.\n",
    "print(\"NaN actual_participants:\", df_total[\"actual_participants\"].isna().sum())\n",
    "print(\"Strange decimals:\", (df_total[\"actual_participants\"].dropna() % 1 != 0).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Inspection: if there are decimals, I locate the rows to see where they come from\n",
    "# (I'm especially interested in the source file year and the academic_year)\n",
    "mask_dec = (df_total[\"actual_participants\"] % 1 != 0)\n",
    "df_total.loc[mask_dec, [\"actual_participants\", \"source_file_year\", \"academic_year\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Cleaning: convert impossible values to null and leave the column as nullable integer\n",
    "# - If it has decimals, I consider it invalid (you can't have 4.7 participants)\n",
    "# - If it's 0, I also consider it invalid (in this dataset 0 participants doesn't make sense)\n",
    "col = \"actual_participants\"\n",
    "\n",
    "mask_nonint = df_total[col].notna() & ((df_total[col] % 1) != 0)\n",
    "mask_zero = (df_total[col] == 0)\n",
    "\n",
    "df_total.loc[mask_nonint | mask_zero, col] = pd.NA\n",
    "df_total[col] = df_total[col].astype(\"Int64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before cleaning age, I do a quick review of extreme values.\n",
    "# The idea is to convert \"participant_age\" to numeric in an auxiliary variable (age_num)\n",
    "# to be able to detect impossible ages (<= 0) or too high (>= 65).\n",
    "\n",
    "# 1) Convert to numeric\n",
    "age_num = pd.to_numeric(\n",
    "    df_total[\"participant_age\"].replace({\"-\": pd.NA, \"\": pd.NA, \"0\": 0}),\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# 2) Review values <= 0 (negative ages or zero don't make sense)\n",
    "print(\"<= 0 (count):\", (age_num <= 0).sum())\n",
    "print(df_total.loc[age_num <= 0, [\"participant_age\", \"academic_year\", \"source_file_year\"]].head(20))\n",
    "\n",
    "# 3) Review values >= 65 (in this analysis I consider them extreme / unrealistic values)\n",
    "print(\"\\n>= 65 (count):\", (age_num >= 65).sum())\n",
    "print(df_total.loc[age_num >= 65, [\"participant_age\", \"academic_year\", \"source_file_year\"]].head(20))\n",
    "\n",
    "# 4) Quick distribution of those extremes to see what values appear most frequently\n",
    "print(\"\\nValue counts (<=0):\")\n",
    "print(df_total.loc[age_num <= 0, \"participant_age\"].value_counts(dropna=False).head(20))\n",
    "\n",
    "print(\"\\nValue counts (>=65):\")\n",
    "print(df_total.loc[age_num >= 65, \"participant_age\"].value_counts(dropna=False).head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I do the final cleaning of \"participant_age\".\n",
    "# 1) Convert to numeric so that what is not a number becomes NaN.\n",
    "# 2) Mark as null values outside the range I consider reasonable for the analysis.\n",
    "# 3) Save the result as nullable integer (Int64), to allow nulls without converting to float.\n",
    "\n",
    "age = pd.to_numeric(\n",
    "    df_total[\"participant_age\"].replace({\"-\": pd.NA, \"\": pd.NA}),\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Range I consider valid: 10 to 65 years\n",
    "# What's outside is left as NA to not invent data\n",
    "age = age.mask((age < 10) | (age > 65))\n",
    "\n",
    "df_total[\"participant_age\"] = age.astype(\"Int64\")\n",
    "\n",
    "# Quick check: number of nulls and most frequent ages\n",
    "print(\"NaN in participant_age:\", df_total[\"participant_age\"].isna().sum())\n",
    "print(df_total[\"participant_age\"].value_counts(dropna=False).head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleaning of \"mobility_duration\" (mobility duration in days).\n",
    "# Here I focus on a clear error: durations = 0, which don't make sense for a real mobility.\n",
    "# First I quantify how many zeros there are, then I convert them to null (NA) and generate a quick summary.\n",
    "\n",
    "col = \"mobility_duration\"\n",
    "\n",
    "# 1) Count of zeros (impossible values or registration errors)\n",
    "n0 = (df_total[col] == 0).sum()\n",
    "print(f\"Duration = 0 days: {n0:,} ({(n0 / len(df_total)) * 100:.4f}%) -> converted to NA\")\n",
    "\n",
    "# 2) Replace 0 with NA\n",
    "df_total.loc[df_total[col] == 0, col] = pd.NA\n",
    "\n",
    "# 3) Quick summary after cleaning\n",
    "print(\"Nulls after cleaning:\", df_total[col].isna().sum())\n",
    "print(\"Min/Median/Max:\", df_total[col].min(), df_total[col].median(), df_total[col].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional review: very long durations.\n",
    "# I don't remove them automatically, but I quantify them to know if there are extreme values\n",
    "# (for example, mobilities of more than one year or more than two years).\n",
    "\n",
    "col = \"mobility_duration\"\n",
    "\n",
    "print(\">365 days:\", (df_total[col] > 365).sum())\n",
    "print(\">730 days:\", (df_total[col] > 730).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In several text columns there are values that actually mean \"unknown\" or \"not applicable\",\n",
    "# but come written as strings (for example \"unknown\", \"n/a\", \"???\", \"-\").\n",
    "# To be able to work better with the data, I unify all those cases and convert them to NA.\n",
    "# This way, null analyses and subsequent cleanups are more consistent.\n",
    "\n",
    "# Typical \"hidden null\" tokens (I don't include \"0\" by default because in some columns it can be a valid value)\n",
    "missing_tokens = {\n",
    "    \"unknown\", \"not specified\", \"none\", \"n/a\", \"na\",\n",
    "    \"?\", \"??\", \"???\", \"????\",\n",
    "    \"-\", \"--\", \"---\",\n",
    "    \"_\", \"__\",\n",
    "    \"? unknown ?\", \"??? - ? unknown ?\"\n",
    "}\n",
    "\n",
    "# Select text type columns (object or category)\n",
    "text_cols = df_total.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "\n",
    "# General replacement in text columns:\n",
    "# - remove spaces\n",
    "# - convert to lowercase to compare with tokens\n",
    "# - convert to NA when it's empty string or matches a missing token\n",
    "for c in text_cols:\n",
    "    s = df_total[c].astype(\"string\").str.strip()\n",
    "    s_low = s.str.lower()\n",
    "    df_total[c] = s.mask((s == \"\") | (s_low.isin(missing_tokens)), pd.NA)\n",
    "\n",
    "# Specific case: in \"receiving_city\" the string \"0\" appears as invalid value in some records.\n",
    "# In this dataset I treat it as null to not confuse it with a real city.\n",
    "if \"receiving_city\" in df_total.columns:\n",
    "    df_total.loc[\n",
    "        df_total[\"receiving_city\"].astype(\"string\").str.strip() == \"0\",\n",
    "        \"receiving_city\"\n",
    "    ] = pd.NA\n",
    "\n",
    "# Quick summary: I show the columns with most null values to have an overview\n",
    "na_counts = df_total.isna().sum().sort_values(ascending=False)\n",
    "print(\"Top 10 columns with most NA:\")\n",
    "print(na_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In \"participant_profile\" I find values in different formats (for example in uppercase).\n",
    "# To avoid \"LEARNERS\" and \"Learner\" being treated as different categories, I unify labels.\n",
    "# Then I convert the column to category type so it's more organized and uses less memory.\n",
    "\n",
    "df_total[\"participant_profile\"] = df_total[\"participant_profile\"].replace({\n",
    "    \"LEARNERS\": \"Learner\",\n",
    "    \"STAFF\": \"Staff\"\n",
    "})\n",
    "\n",
    "df_total[\"participant_profile\"] = df_total[\"participant_profile\"].astype(\"category\")\n",
    "\n",
    "# Quick check: I see how many records there are of each profile (including nulls)\n",
    "print(df_total[\"participant_profile\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The column \"fewer_opportunities\" comes encoded as 0/1 (sometimes as text and sometimes as number).\n",
    "# To make it easier to interpret, I convert it to labels \"No\" and \"Yes\".\n",
    "# Then I convert it to category type, since it only has a few possible categories.\n",
    "\n",
    "df_total[\"fewer_opportunities\"] = df_total[\"fewer_opportunities\"].replace({\n",
    "    \"0\": \"No\", 0: \"No\",\n",
    "    \"1\": \"Yes\", 1: \"Yes\"\n",
    "}).astype(\"category\")\n",
    "\n",
    "# Quick check: distribution of values (including nulls)\n",
    "print(df_total[\"fewer_opportunities\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# From \"education_level\" I want to extract the ISCED level (if it appears in the text).\n# In the dataset it usually comes in formats like:\n# \"ISCED-6 - First cycle / Bachelor's ...\" or similar.\n# The idea is to keep only the number (1..9) to be able to group and filter better.\n\n# 1) Extract the ISCED number (if it exists)\ndf_total[\"isced_level\"] = (\n    df_total[\"education_level\"]\n      .astype(\"string\")                          # ensure string format to work with .str\n      .str.extract(r\"ISCED-(\\d)\", expand=False)  # capture the digit after \"ISCED-\"\n      .astype(\"Int64\")                           # leave it as nullable integer (allows NA)\n)\n\n# 2) Create a simplified group (more useful for project analysis)\n# - HE (6-8): higher education (Bachelor/Master/Doctorate)\n# - Pre-tertiary (1-5): previous levels\n# - ISCED-9 / Other: other cases\n# If ISCED cannot be extracted, it remains as NA.\ndf_total[\"isced_group\"] = pd.Series(pd.NA, index=df_total.index, dtype=\"string\")\n\ndf_total.loc[df_total[\"isced_level\"].between(6, 8), \"isced_group\"] = \"HE (6-8)\"\ndf_total.loc[df_total[\"isced_level\"].between(1, 5), \"isced_group\"] = \"Pre-tertiary (1-5)\"\ndf_total.loc[df_total[\"isced_level\"] == 9, \"isced_group\"] = \"ISCED-9 / Other\"\n\ndf_total[\"isced_group\"] = df_total[\"isced_group\"].astype(\"category\")\n\n# Quick check: distribution of ISCED levels and created groups\nprint(df_total[\"isced_level\"].value_counts(dropna=False).sort_index())\nprint(df_total[\"isced_group\"].value_counts(dropna=False))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The column \"activity_mob\" mixes long text with a code at the beginning (in many cases).\n",
    "# To analyze it better, I separate:\n",
    "# 1) An \"activity_code\" (if it exists) extracted from the beginning of the text.\n",
    "# 2) An \"activity_group\" with more general categories (HE, VET, Youth, etc.).\n",
    "# This allows me to filter and summarize without depending on hundreds of different labels.\n",
    "\n",
    "# Clean spaces to avoid problems when extracting the code\n",
    "s = df_total[\"activity_mob\"].astype(\"string\").str.strip()\n",
    "\n",
    "# 1) Extract the code from the format \"CODE - description\"\n",
    "# Typical examples: \"HE-SMS - ...\", \"LM-VET - ...\", etc.\n",
    "code = s.str.extract(r\"^([A-Z]{1,4}(?:-[A-Z0-9]{1,10})+)\\s*-\\s*\", expand=False)\n",
    "df_total[\"activity_code\"] = code.astype(\"category\")\n",
    "\n",
    "\n",
    "def group_from_activity(text, code):\n",
    "    \"\"\"\n",
    "    Assign a general category to each activity_mob record.\n",
    "    First try to classify by code (if available).\n",
    "    If there is no code or format is old, try to classify by text keywords.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return pd.NA\n",
    "\n",
    "    t = str(text).lower()\n",
    "    c = str(code) if pd.notna(code) else \"\"\n",
    "\n",
    "    # 1) Classification by code (when it exists)\n",
    "    if c:\n",
    "        if c.startswith(\"HE-\"):\n",
    "            return \"HE\"\n",
    "        if c.startswith(\"LM-\") and \"VET\" in c:\n",
    "            return \"VET\"\n",
    "        if c.startswith(\"LM-\") and (\"EXCH\" in c or \"YOU\" in c):\n",
    "            return \"Youth/Volunteering\"\n",
    "        if c.startswith(\"LM-\") and \"PUPIL\" in c:\n",
    "            return \"School\"\n",
    "        if c.startswith(\"LM-\") and \"ADULT\" in c:\n",
    "            return \"Adult\"\n",
    "        if c.startswith(\"SM-\") or c.startswith(\"OA-\"):\n",
    "            return \"Staff/Training\"\n",
    "\n",
    "    # 2) Classification by text (for old cases or without code)\n",
    "    if (\n",
    "        \"student mobility for studies\" in t\n",
    "        or \"student mobility for traineeships\" in t\n",
    "        or \"higher education\" in t\n",
    "    ):\n",
    "        return \"HE\"\n",
    "\n",
    "    if (\n",
    "        \"youth exchanges\" in t\n",
    "        or \"mobility of youth workers\" in t\n",
    "        or \"european voluntary service\" in t\n",
    "        or \"volunteering\" in t\n",
    "    ):\n",
    "        return \"Youth/Volunteering\"\n",
    "\n",
    "    if (\n",
    "        \"vet learners\" in t\n",
    "        or \"mobility of vet learners\" in t\n",
    "        or \"erasmuspro\" in t\n",
    "        or \"vet\" in t\n",
    "    ):\n",
    "        return \"VET\"\n",
    "\n",
    "    if (\n",
    "        \"staff mobility\" in t\n",
    "        or \"staff training\" in t\n",
    "        or \"structured courses/training events\" in t\n",
    "        or \"training/teaching assignments abroad\" in t\n",
    "        or \"teaching/training assignments abroad\" in t\n",
    "        or \"job shadowing\" in t\n",
    "    ):\n",
    "        return \"Staff/Training\"\n",
    "\n",
    "    if \"school pupils\" in t or \"pupil\" in t:\n",
    "        return \"School\"\n",
    "\n",
    "    if \"adult learners\" in t:\n",
    "        return \"Adult\"\n",
    "\n",
    "    # Visits/preparation/hosting (includes Advance Planning Visit)\n",
    "    if (\n",
    "        \"advance planning visit\" in t\n",
    "        or \"preparatory visit\" in t\n",
    "        or \"invited experts\" in t\n",
    "        or \"hosting teachers\" in t\n",
    "    ):\n",
    "        return \"Staff/Training\"\n",
    "\n",
    "    # If it doesn't match anything, I leave it as \"Other\"\n",
    "    return \"Other\"\n",
    "\n",
    "\n",
    "# Apply the function to create the final group.\n",
    "# Use zip to pass both the text and the extracted code at once.\n",
    "df_total[\"activity_group\"] = [\n",
    "    group_from_activity(txt, c)\n",
    "    for txt, c in zip(df_total[\"activity_mob\"], df_total[\"activity_code\"])\n",
    "]\n",
    "\n",
    "df_total[\"activity_group\"] = df_total[\"activity_group\"].astype(\"category\")\n",
    "\n",
    "# Quick check: distribution of groups\n",
    "print(df_total[\"activity_group\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block serves to group \"field_of_education\" into broad categories (ISCED-F broad fields).\n",
    "# In the dataset, this column can come with:\n",
    "# - a code at the beginning (for example \"0410 - Business and administration\")\n",
    "# - or directly text (sometimes without code)\n",
    "# What I do is:\n",
    "# 1) Try to extract a code if it exists at the beginning (4 digits / 2 digits / 1 digit)\n",
    "# 2) If there's no code, apply rules by keywords to assign a macro-field\n",
    "# 3) If it doesn't match anything, I mark it as \"99 - Not classified\"\n",
    "#\n",
    "# Note: many records may come with NA in field_of_education, that's why a high number of NaN appears.\n",
    "\n",
    "COL = \"field_of_education\"  # adjust if your column has spaces\n",
    "\n",
    "# Broad labels (2 digits) for ISCED broad fields\n",
    "broad_labels = {\n",
    "    \"00\": \"00 - Generic programmes and qualifications\",\n",
    "    \"01\": \"01 - Education\",\n",
    "    \"02\": \"02 - Arts and humanities\",\n",
    "    \"03\": \"03 - Social sciences, journalism and information\",\n",
    "    \"04\": \"04 - Business, administration and law\",\n",
    "    \"05\": \"05 - Natural sciences, mathematics and statistics\",\n",
    "    \"06\": \"06 - Information and Communication Technologies\",\n",
    "    \"07\": \"07 - Engineering, manufacturing and construction\",\n",
    "    \"08\": \"08 - Agriculture, forestry, fisheries and veterinary\",\n",
    "    \"09\": \"09 - Health and welfare\",\n",
    "    \"10\": \"10 - Services\",\n",
    "    \"99\": \"99 - Not classified\",\n",
    "}\n",
    "\n",
    "# Rules by text: if the description contains certain keywords, I assign a macro-code\n",
    "rules = [\n",
    "    (r\"\\beducation\\b|teacher training|pre-?school\", \"01\"),\n",
    "    (r\"arts?|fine arts|handicrafts|music|performing|audio-visual|fashion|design|humanities|\"\n",
    "     r\"history|archaeology|philosophy|ethics|religion|theology|languages?|linguistics|literature\", \"02\"),\n",
    "    (r\"social and behavioural|economics|political|psychology|sociology|cultural studies|\"\n",
    "     r\"journalism|reporting|library|archival\", \"03\"),\n",
    "    (r\"accounting|taxation|finance|banking|insurance|management|administration|marketing|advertising|\"\n",
    "     r\"secretarial|office work|wholesale|retail|business\\b|law\\b|legal|juris\", \"04\"),\n",
    "    (r\"biolog|biochem|chemistry|earth sciences|physics|physical sciences|mathematics|statistics|\"\n",
    "     r\"natural sciences|environment\", \"05\"),\n",
    "    (r\"\\bict\\b|information and communication technologies|computer use|database|network|software|applications\", \"06\"),\n",
    "    (r\"engineering|electricity|energy\\b|electronics|automation|mechanics|metal trades|motor vehicles|ships|aircraft|\"\n",
    "     r\"manufacturing|processing|materials|textiles|mining|extraction|architecture|building|civil engineering|construction\", \"07\"),\n",
    "    (r\"agricult|crop|livestock|horticult|forestry|fisheries|veterinar\", \"08\"),\n",
    "    (r\"\\bhealth\\b|medicine|medical|nursing|midwifery|therapy|rehabilitation|pharmacy|dental|\"\n",
    "     r\"welfare|care of the elderly|disabled adults|child care|youth services|social work|counselling\", \"09\"),\n",
    "    (r\"personal services|domestic services|hair|beauty|hotel|restaurants|catering|\"\n",
    "     r\"sports|travel|tourism|leisure|hygiene|occupational health and safety|community sanitation|\"\n",
    "     r\"security services|military|defence|transport|\\bservices\\b\", \"10\"),\n",
    "    (r\"generic programmes|eqf-\\d\", \"00\"),\n",
    "    (r\"^not-?classified\\b\", \"99\"),\n",
    "]\n",
    "\n",
    "def to_macro_code(val):\n",
    "    # If the value is null, I keep it as NA\n",
    "    if pd.isna(val):\n",
    "        return pd.NA\n",
    "\n",
    "    s = str(val).strip().lower()\n",
    "\n",
    "    # 1) If it starts with code, I try to convert it to a 2-digit macro-code\n",
    "    # I accept \"#### - ...\", \"## - ...\" or \"# - ...\"\n",
    "    m = re.match(r\"^(\\d{4}|\\d{2}|\\d)\\s*-\\s*\", s)\n",
    "    if m:\n",
    "        raw = m.group(1)\n",
    "        if len(raw) == 4:\n",
    "            code2 = raw[:2]\n",
    "        elif len(raw) == 2:\n",
    "            code2 = raw\n",
    "        else:\n",
    "            code2 = f\"0{raw}\"\n",
    "\n",
    "        if code2 in broad_labels:\n",
    "            return code2\n",
    "\n",
    "    # 2) If there's no code, I apply keyword rules\n",
    "    for pat, code in rules:\n",
    "        if re.search(pat, s):\n",
    "            return code\n",
    "\n",
    "    # 3) If it doesn't match, I mark it as not classified\n",
    "    return \"99\"\n",
    "\n",
    "\n",
    "# Apply the function to generate the macro-code and its label\n",
    "df_total[\"isced_macro_code\"] = df_total[COL].apply(to_macro_code)\n",
    "df_total[\"isced_macro\"] = df_total[\"isced_macro_code\"].map(broad_labels).astype(\"category\")\n",
    "\n",
    "# Quick summary to see if the classification makes sense\n",
    "print(df_total[\"isced_macro\"].value_counts(dropna=False))\n",
    "print(\"Not classified (99):\", (df_total[\"isced_macro_code\"] == \"99\").sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# In the dataset there are three columns related to countries:\n# - participant_country: usually comes only as name (without code in front).\n# - sending_country / receiving_country: usually come as \"XX - Country name\" (XX = ISO2).\n#\n# Problem:\n# - The same country can appear with different names (for example \"Turkiye\" vs \"Turkey\"),\n#   or with long variants (\"Iran (Islamic Republic of)\").\n# - In sending/receiving, for the same ISO2 code there may be small differences in the name\n#   (spaces, capitalization, variants), which breaks counts if not unified.\n#\n# Strategy:\n# 1) participant_country: clean spaces and apply an equivalence dictionary (name_map).\n# 2) sending_country and receiving_country: normalize by ISO2 code:\n#    - extract the code and name,\n#    - choose the most frequent name as \"canonical\" per code,\n#    - create auxiliary columns <col>_code and <col>_name,\n#    - rebuild the original column in format \"XX - CanonicalName\".\n# 3) Global standard: force sending and receiving to use the same canonical name by code,\n#    so that a code doesn't have a different name depending on the column.\n# 4) Final check: verify that no codes remain with more than one name.\n\n\n\n# 1) Normalization of participant_country (name only, without ISO2)\nname_map = {\n    \"Turkiye\": \"Turkey\",\n    \"Czechia\": \"Czech Republic\",\n    \"Russian Federation\": \"Russia\",\n    \"Moldova (Republic of)\": \"Moldova\",\n    \"Republic of Moldova\": \"Moldova\",\n    \"China (People's Republic of)\": \"China\",\n    \"Iran (Islamic Republic of)\": \"Iran\",\n    \"Syrian Arab Republic\": \"Syria\",\n    \"Viet Nam\": \"Vietnam\",\n    \"Tanzania (United Republic of)\": \"Tanzania\",\n    \"Korea (Republic of)\": \"South Korea\",\n    \"Korea (Democratic People's Republic of)\": \"North Korea\",\n    \"Cabo Verde\": \"Cape Verde\",\n    \"Aland islands\": \"Aland Islands\",\n    \"United States of America\": \"United States\",\n    \"United States Minor outlying islands\": \"United States Minor Outlying Islands\",\n    \"St Lucia\": \"Saint Lucia\",\n    \"St Kitts and Nevis\": \"Saint Kitts and Nevis\",\n    \"St Vincent and the Grenadines\": \"Saint Vincent and the Grenadines\",\n    \"Isle Of Man\": \"Isle of Man\",\n    \"The Republic of North Macedonia\": \"North Macedonia\",\n    \"Kosovo * UN resolution\": \"Kosovo\",\n    \"Congo (Democratic Republic of)\": \"Democratic Republic of the Congo\",\n    \"Lao (People's Democratic Republic)\": \"Laos\",\n    \"Lao People's Democratic Republic\": \"Laos\",\n    \"Brunei Darussalam\": \"Brunei\",\n    \"French Southern and Antarctic Territories\": \"French Southern Territories\",\n}\n\ndf_total[\"participant_country\"] = (\n    df_total[\"participant_country\"]\n      .astype(\"string\")\n      .str.strip()\n      .str.replace(r\"\\s+\", \" \", regex=True)\n      .replace(name_map)\n      .astype(\"category\")\n)\n\nprint(\"participant_country unique:\", df_total[\"participant_country\"].nunique(dropna=True))\n\n\n# 2) Normalization by ISO2 for sending_country and receiving_country\ndef normalize_country_by_code(df, col):\n    \"\"\"\n    Normalize a column that comes as 'XX - Name' (XX = ISO2).\n    For each code, use the most frequent name as canonical name.\n    \"\"\"\n    s = df[col].astype(\"string\").str.strip()\n\n    # Quick validation: check that the pattern \"XX - ...\" exists\n    has_code = s.str.match(r\"^[A-Z]{2}\\s*[-\\u2013]\\s*\", na=False).any()\n    if not has_code:\n        raise ValueError(f\"{col} does not seem to come as 'XX - Name'. Cannot extract ISO2.\")\n\n    # Extract code and name\n    code = s.str.extract(r\"^([A-Z]{2})\\s*[-\\u2013]\\s*\", expand=False)\n    name = s.str.replace(r\"^[A-Z]{2}\\s*[-\\u2013]\\s*\", \"\", regex=True).str.strip()\n    name = name.str.replace(r\"\\s+\", \" \", regex=True)\n\n    # Canonical name by code: the most frequent\n    canon_name = (\n        pd.DataFrame({\"code\": code, \"name\": name})\n          .dropna()\n          .groupby(\"code\")[\"name\"]\n          .agg(lambda x: x.value_counts().idxmax())\n    )\n\n    # Save auxiliary columns and rebuild the normalized original column\n    df[col + \"_code\"] = code.astype(\"category\")\n    df[col + \"_name\"] = code.map(canon_name).astype(\"category\")\n    df[col] = (\n        df[col + \"_code\"].astype(\"string\") + \" - \" + df[col + \"_name\"].astype(\"string\")\n    ).astype(\"category\")\n\n    return df\n\n\ndf_total = normalize_country_by_code(df_total, \"sending_country\")\ndf_total = normalize_country_by_code(df_total, \"receiving_country\")\n\n\n# 3) Global standard: same name by code in sending and receiving\nboth = pd.concat([\n    df_total[[\"sending_country_code\", \"sending_country_name\"]]\n      .rename(columns={\"sending_country_code\": \"code\", \"sending_country_name\": \"name\"}),\n    df_total[[\"receiving_country_code\", \"receiving_country_name\"]]\n      .rename(columns={\"receiving_country_code\": \"code\", \"receiving_country_name\": \"name\"}),\n], ignore_index=True).dropna()\n\nglobal_name = both.groupby(\"code\")[\"name\"].agg(lambda x: x.value_counts().idxmax())\n\ndf_total[\"sending_country_name\"] = df_total[\"sending_country_code\"].map(global_name).astype(\"category\")\ndf_total[\"receiving_country_name\"] = df_total[\"receiving_country_code\"].map(global_name).astype(\"category\")\n\ndf_total[\"sending_country\"] = (\n    df_total[\"sending_country_code\"].astype(\"string\") + \" - \" + df_total[\"sending_country_name\"].astype(\"string\")\n).astype(\"category\")\n\ndf_total[\"receiving_country\"] = (\n    df_total[\"receiving_country_code\"].astype(\"string\") + \" - \" + df_total[\"receiving_country_name\"].astype(\"string\")\n).astype(\"category\")\n\n\n# 4) Final check: confirm there are no codes with more than one canonical name\nfor base in [\"sending_country\", \"receiving_country\"]:\n    tmp = df_total[[base + \"_code\", base + \"_name\"]].dropna()\n    multi = tmp.groupby(base + \"_code\")[base + \"_name\"].nunique()\n    print(\"FINAL\", base, \"codes with >1 name:\", int((multi > 1).sum()))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Reduce spelling variations in city names (extra spaces, suffixes like CEDEX,\n",
    "#   punctuation, districts like \"Paris 16\", accents, etc.).\n",
    "# - Unify some frequent variants/exonyms to a standard English form.\n",
    "# Note on encoding:\n",
    "# - In some records the character \"�\" may appear. This usually indicates a problem\n",
    "#   with encoding already present in the original data. It's not corrected here but quantified as a quality check.\n",
    "\n",
    "def strip_accents(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove accents/diacritics to reduce variants of the same name.\n",
    "    Example: \"Málaga\" -> \"Malaga\".\n",
    "    \"\"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    return s.replace(\"ß\", \"ss\")\n",
    "\n",
    "\n",
    "# Map of frequent exonyms/variants to a standard English form\n",
    "english_map = {\n",
    "    \"Wien\": \"Vienna\",\n",
    "    \"Praha\": \"Prague\",\n",
    "    \"Bruxelles\": \"Brussels\",\n",
    "    \"Roma\": \"Rome\",\n",
    "    \"Milano\": \"Milan\",\n",
    "    \"Lisboa\": \"Lisbon\",\n",
    "    \"Warszawa\": \"Warsaw\",\n",
    "    \"Oporto\": \"Porto\",\n",
    "    \"Bologne\": \"Bologna\",\n",
    "    \"Bolonia\": \"Bologna\",\n",
    "    \"Firenze\": \"Florence\",\n",
    "    \"Torino\": \"Turin\",\n",
    "    \"Kobenhavn\": \"Copenhagen\",\n",
    "    \"Sevilla\": \"Seville\",\n",
    "}\n",
    "\n",
    "\n",
    "def clean_city_series(series: pd.Series, english_map: dict) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Basic cleaning of city names.\n",
    "    Apply simple rules to homogenize text without normalizing city by city.\n",
    "    \"\"\"\n",
    "    s = series.astype(\"string\").str.strip()\n",
    "\n",
    "    # 1) Normalize spaces\n",
    "    s = s.str.replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "    # 2) Remove suffixes like \"CEDEX\" (common in French addresses)\n",
    "    s = s.str.replace(r\"\\s+cedex\\s*\\d*\\s*$\", \"\", regex=True, case=False)\n",
    "\n",
    "    # 3) Remove typical final punctuation\n",
    "    s = s.str.replace(r\"[.,;:?]+$\", \"\", regex=True)\n",
    "\n",
    "    # 4) Remove district numbers at the end (Paris 16, Dublin 2, etc.)\n",
    "    s = s.str.replace(r\"\\s+\\d{1,3}\\s*$\", \"\", regex=True)\n",
    "\n",
    "    # 5) Remove accents/diacritics and standardize capitalization\n",
    "    s = s.apply(lambda x: strip_accents(str(x)) if pd.notna(x) else pd.NA)\n",
    "    s = s.str.title()\n",
    "\n",
    "    # 6) Unify frequent exonyms/variants\n",
    "    s = s.replace(english_map)\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "# Apply cleaning to both city columns if they exist in the DataFrame\n",
    "city_cols = [\"receiving_city\", \"sending_city\"]\n",
    "\n",
    "for col in city_cols:\n",
    "    if col in df_total.columns:\n",
    "        df_total[col] = clean_city_series(df_total[col], english_map).astype(\"category\")\n",
    "\n",
    "        # Light check: number of unique and presence of mojibake (�)\n",
    "        print(f\"\\n{col}\")\n",
    "        print(\"Unique:\", df_total[col].nunique(dropna=True))\n",
    "\n",
    "        bad = df_total[col].astype(\"string\").str.contains(\"�\", na=False).sum()\n",
    "        print(\"Rows with mojibake (�):\", bad)\n",
    "\n",
    "        # Minimal examples (only if mojibake exists) to be able to inspect it without generating too much output\n",
    "        if bad > 0:\n",
    "            print(\"Examples with mojibake:\")\n",
    "            print(df_total.loc[df_total[col].astype(\"string\").str.contains(\"�\", na=False), col].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going to prepare a filtered dataset for analysis (which I'll then export to Power BI).\n",
    "# Steps:\n",
    "# 1) Create year_start from academic_year (for example \"2019-20\" -> 2019).\n",
    "# 2) Filter the period 2017–2024 by start year.\n",
    "# 3) Compare criteria to define HE (ISCED, activity_group, field) as evidence.\n",
    "# 4) Define the final HE dataset by ISCED 6–8 and add an he_strict flag.\n",
    "\n",
    "df = df_total.copy()\n",
    "\n",
    "df[\"year_start\"] = pd.to_numeric(\n",
    "    df[\"academic_year\"].astype(\"string\").str.extract(r\"(\\d{4})\")[0],\n",
    "    errors=\"coerce\"\n",
    ").astype(\"Int64\")\n",
    "\n",
    "df_17_24 = df[df[\"year_start\"].between(2017, 2024)].copy()\n",
    "\n",
    "print(df_17_24[\"academic_year\"].value_counts().sort_index().tail(10))\n",
    "print(\"Rows:\", len(df_17_24))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Comparison of criteria to define HE (evidence)\nmask_isced = df_17_24[\"isced_level\"].isin([6, 7, 8])\nmask_act   = df_17_24[\"activity_group\"].eq(\"HE\")\nmask_field = df_17_24[\"field\"].eq(\"Higher Education\")\n\nsummary = pd.DataFrame({\n    \"Filter\": [\n        \"ISCED 6-8 (base criterion)\",\n        \"activity_group == HE\",\n        \"field == Higher Education\",\n        \"ISCED 6-8 AND activity_group HE (HE strict)\",\n        \"ISCED 6-8 AND field HE\",\n        \"field HE AND activity_group HE\",\n        \"ISCED 6-8 AND field HE AND activity_group HE\",\n    ],\n    \"Rows\": [\n        mask_isced.sum(),\n        mask_act.sum(),\n        mask_field.sum(),\n        (mask_isced & mask_act).sum(),\n        (mask_isced & mask_field).sum(),\n        (mask_field & mask_act).sum(),\n        (mask_isced & mask_field & mask_act).sum(),\n    ]\n})\n\nsummary[\"% over df_17_24\"] = (summary[\"Rows\"] / len(df_17_24) * 100).round(2)\nprint(summary)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final HE dataset for Power BI (HE by ISCED 6–8 + he_strict flag)\n",
    "df_17_24[\"he_isced\"] = mask_isced\n",
    "df_17_24[\"he_strict\"] = mask_isced & (df_17_24[\"activity_group\"] == \"HE\")\n",
    "\n",
    "df_he = df_17_24[df_17_24[\"he_isced\"]].copy()\n",
    "\n",
    "print(\"HE (ISCED 6-8):\", len(df_he))\n",
    "print(\"HE strict (within HE):\", df_he[\"he_strict\"].sum())\n",
    "print(df_he[\"academic_year\"].value_counts().sort_index().tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_he.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataset for Power BI: I keep only the columns I'm going to use in the analysis.\n",
    "# For countries, I keep ISO2 (code) + name (name) because it's more robust for maps and for a star model.\n",
    "\n",
    "cols_keep = [\n",
    "    \"academic_year\", \"year_start\",\n",
    "    \"mobility_start_ym\", \"mobility_start_year\", \"mobility_start_month_num\",\n",
    "\n",
    "    # Countries (ISO2 + name)\n",
    "    \"sending_country_code\", \"sending_country_name\",\n",
    "    \"receiving_country_code\", \"receiving_country_name\",\n",
    "\n",
    "    # participant_country comes as name (not ISO2 in this pipeline)\n",
    "    \"participant_country\",\n",
    "\n",
    "    # Demographic variables / flags\n",
    "    \"participant_gender\", \"participant_profile\", \"fewer_opportunities\",\n",
    "\n",
    "    # Numeric metrics\n",
    "    \"participant_age\", \"mobility_duration\", \"actual_participants\",\n",
    "\n",
    "    # Education and activity\n",
    "    \"isced_level\", \"isced_group\",\n",
    "    \"activity_group\",\n",
    "    \"isced_macro\",\n",
    "\n",
    "    # Flag for conservative analysis within HE\n",
    "    \"he_strict\",\n",
    "]\n",
    "\n",
    "df_he_pbi = df_he[cols_keep].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# I export the final dataset in two formats:\n# - Parquet (recommended): compressed, fast to load, preserves data types.\n# - CSV (universal): for compatibility with tools that don't support Parquet.\n# The output folder is created if it doesn't exist.\n\nimport os\n\noutput_dir = \"data/processed\"\nos.makedirs(output_dir, exist_ok=True)\n\n# 1) Export to Parquet (preferred for Power BI)\nparquet_path = os.path.join(output_dir, \"erasmus_he_2017_2024.parquet\")\ndf_he_pbi.to_parquet(parquet_path, index=False)\n\n# 2) Export to CSV (fallback)\ncsv_path = os.path.join(output_dir, \"erasmus_he_2017_2024.csv\")\ndf_he_pbi.to_csv(csv_path, index=False)\n\n# Quick check: confirm export\nprint(f\"Exported {len(df_he_pbi):,} records to {output_dir}/\")\nprint(f\"  - erasmus_he_2017_2024.parquet ({os.path.getsize(parquet_path) / 1e6:.1f} MB)\")\nprint(f\"  - erasmus_he_2017_2024.csv ({os.path.getsize(csv_path) / 1e6:.1f} MB)\")"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}