{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Resumen Ejecutivo del Pipeline\n",
    "\n",
    "### Transformación Completa\n",
    "\n",
    "| Métrica | Antes | Después | Cambio |\n",
    "|---------|-------|---------|--------|\n",
    "| **Registros totales** | 5,955,075 | 2,082,071 | Filtrado HE 2017-2024 |\n",
    "| **Duplicados** | 9,165 (0.15%) | 0 | ✓ Eliminados |\n",
    "| **Archivos fuente** | 11 CSVs (2014-2024) | 1 dataset unificado | ✓ Consolidado |\n",
    "| **Columnas** | 19-21 (inconsistente) | 26 (estandarizadas) | ✓ Normalizado |\n",
    "| **Formato de fechas** | Mixto | ISO 8601 | ✓ Estandarizado |\n",
    "| **Nombres de países** | Múltiples variantes | ISO2 + nombre canónico | ✓ Unificado |\n",
    "\n",
    "### Calidad de Datos\n",
    "\n",
    " **100% de países normalizados** por código ISO2\n",
    " **0 duplicados exactos**\n",
    " **621,084 edades outliers** convertidas a NA (outliers imposibles <10 o >65)\n",
    " **16,069 duraciones = 0** convertidas a NA\n",
    " **Tokens de nulos** unificados (\"unknown\", \"n/a\", \"???\", etc. → NA)\n",
    "\n",
    "### Dataset Final para Power BI\n",
    "\n",
    "- **Enfoque**: Educación Superior (ISCED 6-8)\n",
    "- **Período**: 2017-2024 (años de inicio de movilidad)\n",
    "- **Variables**: 26 columnas analíticas\n",
    "- **Formatos**: Parquet (comprimido) + CSV (universal)\n",
    "\n",
    "---\n",
    "\n",
    "Para detalles técnicos del proceso de limpieza, ver [`PIPELINE.md`](PIPELINE.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos las librerías necesarias para el proceso\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-HYIXqgm7fbg",
    "outputId": "f0c34fb1-96f6-4686-8532-a340fccdc02f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando estructura de archivos...\n",
      "  2014: 20 columnas (encoding=utf-8)\n",
      "  2015: 20 columnas (encoding=utf-8)\n",
      "  2016: 20 columnas (encoding=utf-8)\n",
      "  2017: 20 columnas (encoding=utf-8)\n",
      "  2018: 20 columnas (encoding=utf-8)\n",
      "  2019: 20 columnas (encoding=utf-8)\n",
      "  2020: 19 columnas (encoding=utf-8)\n",
      "  2021: 19 columnas (encoding=utf-8)\n",
      "  2022: 19 columnas (encoding=utf-8)\n",
      "  2023: 19 columnas (encoding=utf-8)\n",
      "  2024: 19 columnas (encoding=utf-8)\n",
      "\n",
      "Comparando columnas entre años...\n",
      "\n",
      "  Diferencias detectadas en 2015 (base=2014):\n",
      "    - Faltan: {'sending_organisation', 'mobility_duration_-_calendar_days', 'receiving_organisation'}\n",
      "    - Nuevas: {'mobility_duration', 'sending_organization', 'receiving_organization'}\n",
      "\n",
      "  Diferencias detectadas en 2016 (base=2014):\n",
      "    - Faltan: {'sending_organisation', 'mobility_duration_-_calendar_days', 'receiving_organisation'}\n",
      "    - Nuevas: {'mobility_duration', 'sending_organization', 'receiving_organization'}\n",
      "\n",
      "  Diferencias detectadas en 2017 (base=2014):\n",
      "    - Faltan: {'sending_organisation', 'mobility_duration_-_calendar_days', 'receiving_organisation'}\n",
      "    - Nuevas: {'mobility_duration', 'sending_organization', 'receiving_organization'}\n",
      "\n",
      "  Diferencias detectadas en 2018 (base=2014):\n",
      "    - Faltan: {'mobility_start_year/month', 'sending_organisation', 'mobility_duration_-_calendar_days', 'receiving_organisation'}\n",
      "    - Nuevas: {'mobility_duration', 'sending_organization', 'receiving_organization', 'mobility_start_month'}\n",
      "\n",
      "  Diferencias detectadas en 2019 (base=2014):\n",
      "    - Faltan: {'sending_organisation', 'mobility_duration_-_calendar_days', 'receiving_organisation'}\n",
      "    - Nuevas: {'mobility_duration', 'sending_organization', 'receiving_organization'}\n",
      "\n",
      "  Diferencias detectadas en 2020 (base=2014):\n",
      "    - Faltan: {'mobility_start_year/month', 'actual_participants', 'mobility_duration_-_calendar_days', 'project_reference', 'sending_organisation', 'receiving_organisation'}\n",
      "    - Nuevas: {'mobility_duration', 'mobility_start_month', 'actual_participants_(contracted_projects)', 'sending_organization', 'receiving_organization'}\n",
      "\n",
      "  Diferencias detectadas en 2021 (base=2014):\n",
      "    - Faltan: {'mobility_duration_-_calendar_days', 'project_reference'}\n",
      "    - Nuevas: {'mobility_duration'}\n",
      "\n",
      "  Diferencias detectadas en 2022 (base=2014):\n",
      "    - Faltan: {'mobility_start_year/month', 'actual_participants', 'mobility_duration_-_calendar_days', 'project_reference', 'sending_organisation', 'receiving_organisation'}\n",
      "    - Nuevas: {'mobility_duration', 'mobility_start_month', 'actual_participants_(contracted_projects)', 'sending_organization', 'receiving_organization'}\n",
      "\n",
      "  Diferencias detectadas en 2023 (base=2014):\n",
      "    - Faltan: {'mobility_duration_-_calendar_days', 'project_reference'}\n",
      "    - Nuevas: {'mobility_duration'}\n",
      "\n",
      "  Diferencias detectadas en 2024 (base=2014):\n",
      "    - Faltan: {'mobility_start_year/month', 'actual_participants', 'mobility_duration_-_calendar_days', 'project_reference', 'sending_organisation', 'receiving_organisation'}\n",
      "    - Nuevas: {'mobility_duration', 'mobility_start_month', 'actual_participants_(contracted_projects)', 'sending_organization', 'receiving_organization'}\n",
      "\n",
      "Resumen:\n",
      "  Años OK: [2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n"
     ]
    }
   ],
   "source": [
    "# - Compruebo si los CSV anuales (2014–2024) tienen la misma estructura de columnas.\n",
    "# - Detecto diferencias entre años antes de hacer la carga completa.\n",
    "\n",
    "# - Normalizo nombres de columnas (minúsculas + guiones bajos) para comparar de forma consistente.\n",
    "folder_path = \"bases_datos_erasmus\"\n",
    "years = range(2014, 2025)  # incluye 2024\n",
    "\n",
    "\n",
    "def normalize_columns(columns):\n",
    "    \"\"\"\n",
    "    Normaliza nombres de columnas para comparar entre años.\n",
    "    En este bloque hago una normalización básica (minúsculas y guiones bajos).\n",
    "    \"\"\"\n",
    "    normalized = []\n",
    "    for col in columns:\n",
    "        col = col.strip().lower()\n",
    "        col = re.sub(r\"\\s+\", \" \", col)      \n",
    "        col = col.replace(\" \", \"_\")         \n",
    "        normalized.append(col)\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def read_header_with_fallback(file_path, sep=\";\"):\n",
    "    \"\"\"\n",
    "    Lee SOLO la cabecera del CSV (nrows=0) probando varios encodings.\n",
    "    Devuelve (df_header, encoding_usado).\n",
    "    \"\"\"\n",
    "    encodings_to_try = [\"utf-8\", \"utf-8-sig\", \"latin1\"]\n",
    "    last_error = None\n",
    "\n",
    "    for enc in encodings_to_try:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep=sep, nrows=0, encoding=enc)\n",
    "            return df, enc\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "\n",
    "    raise last_error\n",
    "\n",
    "\n",
    "column_sets = {}   # columnas (normalizadas) por año\n",
    "errors = {}        # errores de lectura por año\n",
    "\n",
    "print(\"Verificando estructura de archivos...\")\n",
    "\n",
    "for year in years:\n",
    "    file_name = f\"Erasmus-KA1-Mobility-Data-{year}.csv\"\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    try:\n",
    "        df_header, used_enc = read_header_with_fallback(file_path, sep=\";\")\n",
    "        column_sets[year] = normalize_columns(df_header.columns)\n",
    "        print(f\"  {year}: {len(df_header.columns)} columnas (encoding={used_enc})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        errors[year] = str(e)\n",
    "        print(f\"  {year}: Error leyendo {file_name}: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\nComparando columnas entre años...\")\n",
    "\n",
    "if not column_sets:\n",
    "    print(\"No se pudo leer ningún archivo. Revisa rutas, nombres y separador.\")\n",
    "else:\n",
    "    # Tomo como referencia el primer año que se leyó correctamente\n",
    "    base_year = min(column_sets.keys())\n",
    "    base_cols = set(column_sets[base_year])\n",
    "\n",
    "    for year in sorted(column_sets.keys()):\n",
    "        if year == base_year:\n",
    "            continue\n",
    "\n",
    "        current_cols = set(column_sets[year])\n",
    "\n",
    "        if current_cols != base_cols:\n",
    "            print(f\"\\n  Diferencias detectadas en {year} (base={base_year}):\")\n",
    "            print(\"    - Faltan:\", base_cols - current_cols)\n",
    "            print(\"    - Nuevas:\", current_cols - base_cols)\n",
    "        else:\n",
    "            print(f\"  {year} tiene las mismas columnas que {base_year}.\")\n",
    "\n",
    "\n",
    "print(\"\\nResumen:\")\n",
    "print(\"  Años OK:\", sorted(column_sets.keys()))\n",
    "if errors:\n",
    "    print(\"  Años con error:\", sorted(errors.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DQA0zaJIB2pv",
    "outputId": "d0e61516-0a45-471e-aef3-b0a260a1aba9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando y unificando archivos por año...\n",
      " 2014 cargado: 235,702 filas, 21 columnas (encoding=utf-8)\n",
      " 2015 cargado: 553,475 filas, 21 columnas (encoding=utf-8)\n",
      " 2016 cargado: 603,003 filas, 21 columnas (encoding=utf-8)\n",
      " 2017 cargado: 643,036 filas, 21 columnas (encoding=utf-8)\n",
      " 2018 cargado: 690,757 filas, 21 columnas (encoding=utf-8)\n",
      " 2019 cargado: 739,730 filas, 21 columnas (encoding=utf-8)\n",
      " 2020 cargado: 276,660 filas, 20 columnas (encoding=utf-8)\n",
      " 2021 cargado: 52,109 filas, 20 columnas (encoding=utf-8)\n",
      " 2022 cargado: 365,106 filas, 20 columnas (encoding=utf-8)\n",
      " 2023 cargado: 842,756 filas, 20 columnas (encoding=utf-8)\n",
      " 2024 cargado: 952,741 filas, 20 columnas (encoding=utf-8)\n",
      "\n",
      "============================================================\n",
      "ARCHIVOS UNIDOS\n",
      "============================================================\n",
      "Total registros: 5,955,075\n",
      "Total columnas:  21\n"
     ]
    }
   ],
   "source": [
    "# En el bloque anterior comprobé que existen diferencias pequeñas entre años en los nombres de columnas.\n",
    "# En este bloque ya cargo los CSV completos y aplico una estrategia de unificación para poder concatenarlos:\n",
    "# - Normalizo nombres de columnas (minúsculas, guiones bajos, etc.)\n",
    "# - Renombro columnas equivalentes a nombres canónicos (RENAME_MAP)\n",
    "# - Elimino columnas residuales tipo \"Unnamed\"\n",
    "# - Añado source_file_year para trazabilidad\n",
    "# - Alineo todas las columnas entre años y concateno en df_total\n",
    "\n",
    "\n",
    "folder_path = \"bases_datos_erasmus\"\n",
    "years = range(2014, 2025)\n",
    "\n",
    "def normalize_columns(cols):\n",
    "    \"\"\"\n",
    "    Normaliza nombres de columnas para que todos los años sigan el mismo formato.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for c in cols:\n",
    "        c = str(c).strip().lower()\n",
    "        c = re.sub(r\"\\s+\", \" \", c)\n",
    "        c = c.replace(\" \", \"_\")\n",
    "        c = c.replace(\"/\", \"_\")\n",
    "        c = re.sub(r\"[()]\", \"\", c)\n",
    "        c = re.sub(r\"__+\", \"_\", c)\n",
    "        out.append(c)\n",
    "    return out\n",
    "\n",
    "# Mapeo a nombres canónicos (columnas equivalentes entre años)\n",
    "RENAME_MAP = {\n",
    "    \"sending_organisation\": \"sending_organization\",\n",
    "    \"receiving_organisation\": \"receiving_organization\",\n",
    "    \"mobility_duration_-_calendar_days\": \"mobility_duration\",\n",
    "    \"mobility_duration_in_days\": \"mobility_duration\",\n",
    "    \"mobility_start_year_month\": \"mobility_start_month\",\n",
    "    \"actual_participants_contracted_projects\": \"actual_participants\",\n",
    "}\n",
    "\n",
    "def read_csv_with_fallback(path, sep=\";\"):\n",
    "    \"\"\"\n",
    "    Lee el CSV probando distintos encodings para evitar fallos de lectura.\n",
    "    Devuelve (df, encoding_usado).\n",
    "    \"\"\"\n",
    "    encodings = [\"utf-8\", \"utf-8-sig\", \"latin1\"]\n",
    "    last_error = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(path, sep=sep, low_memory=False, encoding=enc)\n",
    "            return df, enc\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "    raise last_error\n",
    "\n",
    "df_list = []\n",
    "all_cols = set()\n",
    "\n",
    "print(\"Cargando y unificando archivos por año...\")\n",
    "\n",
    "for year in years:\n",
    "    file_name = f\"Erasmus-KA1-Mobility-Data-{year}.csv\"\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    try:\n",
    "        # 1) Cargo el CSV del año\n",
    "        df, used_enc = read_csv_with_fallback(file_path, sep=\";\")\n",
    "\n",
    "        # 2) Normalizo nombres de columnas y aplico renombrado canónico\n",
    "        df.columns = normalize_columns(df.columns)\n",
    "        df = df.rename(columns=RENAME_MAP)\n",
    "\n",
    "        # 3) Elimino columnas residuales tipo \"Unnamed\"\n",
    "        unnamed_cols = [c for c in df.columns if c.startswith(\"unnamed\")]\n",
    "        if unnamed_cols:\n",
    "            df = df.drop(columns=unnamed_cols)\n",
    "\n",
    "        # 4) Añado trazabilidad del año de origen\n",
    "        df[\"source_file_year\"] = year\n",
    "\n",
    "        # 5) Acumulo DF y columnas para alinear al final\n",
    "        df_list.append(df)\n",
    "        all_cols |= set(df.columns)\n",
    "\n",
    "        print(f\" {year} cargado: {df.shape[0]:,} filas, {df.shape[1]} columnas (encoding={used_enc})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error en {year}: {e}\")\n",
    "\n",
    "if not df_list:\n",
    "    raise ValueError(\"No se pudo cargar ningún archivo. Revisa la ruta, nombres y separador.\")\n",
    "\n",
    "# Orden fijo de columnas para que el dataset final sea estable\n",
    "all_cols = sorted(all_cols)\n",
    "\n",
    "# Alineo todos los años al mismo conjunto de columnas\n",
    "df_list_aligned = [d.reindex(columns=all_cols) for d in df_list]\n",
    "\n",
    "# Concateno todo en df_total\n",
    "df_total = pd.concat(df_list_aligned, ignore_index=True)\n",
    "\n",
    "print(f\"\\n{'='*60}\\nARCHIVOS UNIDOS\\n{'='*60}\")\n",
    "print(f\"Total registros: {df_total.shape[0]:,}\")\n",
    "print(f\"Total columnas:  {df_total.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5955075 entries, 0 to 5955074\n",
      "Data columns (total 21 columns):\n",
      " #   Column                  Dtype  \n",
      "---  ------                  -----  \n",
      " 0   academic_year           object \n",
      " 1   activity_mob            object \n",
      " 2   actual_participants     float64\n",
      " 3   education_level         object \n",
      " 4   fewer_opportunities     object \n",
      " 5   field                   object \n",
      " 6   field_of_education      object \n",
      " 7   mobility_duration       int64  \n",
      " 8   mobility_start_month    object \n",
      " 9   participant_age         object \n",
      " 10  participant_country     object \n",
      " 11  participant_gender      object \n",
      " 12  participant_profile     object \n",
      " 13  project_reference       object \n",
      " 14  receiving_city          object \n",
      " 15  receiving_country       object \n",
      " 16  receiving_organization  object \n",
      " 17  sending_city            object \n",
      " 18  sending_country         object \n",
      " 19  sending_organization    object \n",
      " 20  source_file_year        int64  \n",
      "dtypes: float64(1), int64(2), object(18)\n",
      "memory usage: 954.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_total.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2013-2014' '2014-2015' '2015-2016' '2016-2017' '2017-2018' '2018-2019'\n",
      " '2019-2020' '2020-2021' '2021-22' '2020-21' '2021-2022' '2022-2023'\n",
      " '2022-23' '2023-24' '2024-25' '2023-2024']\n"
     ]
    }
   ],
   "source": [
    "# Antes de normalizar, reviso qué valores distintos aparecen en la columna \"academic_year\".\n",
    "# Esto me sirve para detectar formatos diferentes (por ejemplo: \"2020-2021\" vs \"2020-21\").\n",
    "print(df_total[\"academic_year\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2013-14' '2014-15' '2015-16' '2016-17' '2017-18' '2018-19' '2019-20'\n",
      " '2020-21' '2021-22' '2022-23' '2023-24' '2024-25']\n"
     ]
    }
   ],
   "source": [
    "# Defino una función para dejar todos los valores de \"academic_year\" con el mismo formato.\n",
    "# Objetivo: que siempre quede como \"YYYY-YY\" (por ejemplo, \"2020-21\").\n",
    "def norm_academic_year(x):\n",
    "    # Si el valor es nulo (NaN), lo mantengo como nulo\n",
    "    if pd.isna(x):\n",
    "        return pd.NA\n",
    "\n",
    "    # Paso a string y quito espacios por si vienen valores tipo \" 2020-2021 \"\n",
    "    s = str(x).strip()\n",
    "\n",
    "    # Caso 1: formato completo \"YYYY-YYYY\" (ej. \"2020-2021\")\n",
    "    # En este caso, lo convierto a \"YYYY-YY\" (ej. \"2020-21\")\n",
    "    m = re.fullmatch(r\"(\\d{4})-(\\d{4})\", s)\n",
    "    if m:\n",
    "        return f\"{m.group(1)}-{m.group(2)[-2:]}\"\n",
    "\n",
    "    # Caso 2: formato ya reducido \"YYYY-YY\" (ej. \"2020-21\")\n",
    "    # Si ya está bien, lo dejo tal cual\n",
    "    m = re.fullmatch(r\"(\\d{4})-(\\d{2})\", s)\n",
    "    if m:\n",
    "        return s\n",
    "\n",
    "    # Si aparece algún formato raro que no encaja con los anteriores,\n",
    "    # lo devuelvo igual para poder revisarlo más adelante.\n",
    "    return s\n",
    "\n",
    "\n",
    "# Aplico la normalización y sobrescribo la columna con los valores ya unificados\n",
    "df_total[\"academic_year\"] = df_total[\"academic_year\"].apply(norm_academic_year)\n",
    "\n",
    "# Comprobación rápida: vuelvo a mirar los valores únicos para asegurarme de que quedó homogéneo\n",
    "print(df_total[\"academic_year\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicados exactos: 9,165 (0.1539%)\n",
      "\n",
      "Ejemplo duplicados exactos (5 filas):\n",
      "     academic_year sending_country receiving_city participant_age  \\\n",
      "1078       2013-14    AT - Austria        Demonte              15   \n",
      "1079       2013-14    AT - Austria        Demonte              19   \n",
      "1080       2013-14    AT - Austria        Demonte              15   \n",
      "1081       2013-14    AT - Austria        Demonte              19   \n",
      "1741       2014-15    AT - Austria        TAMPERE              20   \n",
      "\n",
      "      mobility_duration  \n",
      "1078                 28  \n",
      "1079                 28  \n",
      "1080                 28  \n",
      "1081                 28  \n",
      "1741                124  \n",
      "\n",
      "Filas repetidas ignorando 'academic_year': 18,381\n",
      "Grupos repetidos en múltiples años: 96\n",
      "\n",
      "Ejemplo multi-año (5 filas):\n",
      "        academic_year sending_country receiving_city participant_age  \\\n",
      "5584359       2023-24     PL - Poland        SEVILLA              17   \n",
      "5584360       2024-25     PL - Poland        SEVILLA              17   \n",
      "\n",
      "         mobility_duration  \n",
      "5584359                  5  \n",
      "5584360                  5  \n"
     ]
    }
   ],
   "source": [
    "# Este bloque lo uso para revisar duplicados en el dataset final.\n",
    "# Distingo dos casos:\n",
    "# 1) Duplicados exactos: filas idénticas en todas las columnas (incluyendo academic_year).\n",
    "# 2) Filas repetidas ignorando academic_year: mismas características pero en distintos cursos,\n",
    "#    lo cual podría pasar si un registro se repite entre años o si hay una forma de “solapamiento”.\n",
    "col_year = \"academic_year\"\n",
    "\n",
    "# Columnas que muestro como ejemplo cuando encuentro duplicados\n",
    "# (solo para inspección rápida y que el output no sea enorme)\n",
    "cols_show = [\"academic_year\", \"sending_country\", \"receiving_city\", \"participant_age\", \"mobility_duration\"]\n",
    "\n",
    "\n",
    "# Asigno df_total a una variable por comodidad.\n",
    "# No hago copy() para no gastar memoria extra, ya que aquí no modifico el DataFrame.\n",
    "df = df_total\n",
    "\n",
    "\n",
    "# 1) Duplicados exactos (todas las columnas iguales)\n",
    "n = len(df)\n",
    "n_dup = df.duplicated().sum()\n",
    "print(f\"Duplicados exactos: {n_dup:,} ({n_dup / n * 100:.4f}%)\")\n",
    "\n",
    "# Si existen, muestro un ejemplo de algunas filas para ver el patrón\n",
    "if n_dup:\n",
    "    print(\"\\nEjemplo duplicados exactos (5 filas):\")\n",
    "    print(df.loc[df.duplicated(keep=False), cols_show].head(5))\n",
    "\n",
    "\n",
    "# 2) Filas repetidas ignorando academic_year\n",
    "# Primero convierto academic_year a un año numérico (me quedo con el primer año: \"2015-16\" -> 2015)\n",
    "# Esto me sirve solo para analizar si un mismo \"grupo\" aparece en más de un año.\n",
    "year_temp = pd.to_numeric(df[col_year].astype(str).str[:4], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Creo una lista con todas las columnas excepto academic_year para comparar “todo menos el curso”\n",
    "cols_compare = [c for c in df.columns if c != col_year]\n",
    "\n",
    "# Para detectar repetidos de forma eficiente, creo un hash por fila usando esas columnas\n",
    "h = pd.util.hash_pandas_object(df[cols_compare], index=False)\n",
    "\n",
    "# Marco como repetidas las filas cuyo hash aparece más de una vez\n",
    "mask_rep = h.duplicated(keep=False)\n",
    "print(f\"\\nFilas repetidas ignorando '{col_year}': {mask_rep.sum():,}\")\n",
    "\n",
    "# Ahora miro cuántos de esos grupos aparecen en más de un año distinto\n",
    "tmp = pd.DataFrame({\"h\": h[mask_rep], \"y\": year_temp[mask_rep]})\n",
    "multi = tmp.groupby(\"h\")[\"y\"].nunique()\n",
    "\n",
    "# Cuento cuántos hashes tienen más de un año asociado (multi-año)\n",
    "n_multi_groups = (multi > 1).sum()\n",
    "print(f\"Grupos repetidos en múltiples años: {n_multi_groups:,}\")\n",
    "\n",
    "# Si existe algún caso multi-año, muestro un ejemplo para entenderlo mejor\n",
    "if n_multi_groups:\n",
    "    ex_hash = multi[multi > 1].index[0]\n",
    "    print(\"\\nEjemplo multi-año (5 filas):\")\n",
    "    print(df.loc[h == ex_hash, cols_show].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros antes: 5,955,075\n",
      "Duplicados exactos detectados: 9,165 (0.1539%)\n",
      "Registros después: 5,945,910\n",
      "Eliminados: 9,165\n",
      "Duplicados restantes: 0\n"
     ]
    }
   ],
   "source": [
    "# En el análisis anterior vi que había duplicados exactos (filas idénticas en todas las columnas).\n",
    "# Como son registros repetidos al 100%, decido eliminarlos para no contar dos veces la misma observación.\n",
    "# Mantengo la primera aparición de cada fila y elimino el resto.\n",
    "\n",
    "n_before = len(df_total)\n",
    "n_dup = df_total.duplicated().sum()\n",
    "\n",
    "print(f\"Registros antes: {n_before:,}\")\n",
    "print(f\"Duplicados exactos detectados: {n_dup:,} ({n_dup / n_before * 100:.4f}%)\")\n",
    "\n",
    "# Elimino duplicados exactos y reseteo el índice para dejar el DataFrame limpio\n",
    "df_total = df_total.drop_duplicates(keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "n_after = len(df_total)\n",
    "print(f\"Registros después: {n_after:,}\")\n",
    "print(f\"Eliminados: {n_before - n_after:,}\")\n",
    "\n",
    "# Comprobación final: confirmo que ya no quedan duplicados exactos\n",
    "print(\"Duplicados restantes:\", df_total.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mobility_start_month\n",
       "2024-09    168684\n",
       "2019-09    166655\n",
       "2018-09    158946\n",
       "2023-09    155014\n",
       "2017-09    151617\n",
       "2016-09    145404\n",
       "2015-09    138285\n",
       "2014-09    124833\n",
       "2024-04    108741\n",
       "2024-02     93815\n",
       "2024-05     92027\n",
       "2023-10     87645\n",
       "2024-03     83071\n",
       "2024-10     79464\n",
       "2023-05     78943\n",
       "2023-02     76198\n",
       "2020-02     73570\n",
       "2023-03     72509\n",
       "2024-06     70840\n",
       "2019-02     70045\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reviso los valores más frecuentes de \"mobility_start_month\" para entender\n",
    "# por qué la columna está como tipo object (normalmente pasa por formatos mezclados,\n",
    "# valores raros, o porque viene como texto \"YYYY-MM\").\n",
    "df_total[\"mobility_start_month\"].value_counts(dropna=False).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulos mobility_start_ym: 0\n",
      "mobility_start_month_num\n",
      "1      419047\n",
      "2      594997\n",
      "3      478565\n",
      "4      471914\n",
      "5      457854\n",
      "6      374451\n",
      "7      387487\n",
      "8      486931\n",
      "9     1330370\n",
      "10     559639\n",
      "11     281515\n",
      "12     103140\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# No modifico la columna original para poder volver atrás si hace falta.\n",
    "# Creo una columna auxiliar en formato fecha (\"mobility_start_ym\") a partir del texto.\n",
    "# Uso errors=\"coerce\" para que los valores que no encajen con el formato pasen a NaT (nulo de fecha).\n",
    "df_total[\"mobility_start_ym\"] = pd.to_datetime(\n",
    "    df_total[\"mobility_start_month\"].astype(str),\n",
    "    format=\"%Y-%m\",\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# A partir de la fecha, saco el año y el mes como columnas separadas.\n",
    "# Uso Int64 (nullable) para permitir nulos sin que se conviertan en float.\n",
    "df_total[\"mobility_start_year\"] = df_total[\"mobility_start_ym\"].dt.year.astype(\"Int64\")\n",
    "df_total[\"mobility_start_month_num\"] = df_total[\"mobility_start_ym\"].dt.month.astype(\"Int64\")\n",
    "\n",
    "# Comprobaciones rápidas:\n",
    "# 1) Cuántos valores no se pudieron convertir (quedaron como NaT)\n",
    "print(\"Nulos mobility_start_ym:\", df_total[\"mobility_start_ym\"].isna().sum())\n",
    "\n",
    "# 2) Distribución de meses (incluyendo nulos) para ver si tiene sentido\n",
    "print(df_total[\"mobility_start_month_num\"].value_counts(dropna=False).sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN actual_participants: 0\n",
      "Decimales raros: 1\n"
     ]
    }
   ],
   "source": [
    "# 1) Comprobación rápida: cuántos nulos hay y si existen valores con decimales\n",
    "# En teoría \"actual_participants\" debería ser un número entero y > 0.\n",
    "print(\"NaN actual_participants:\", df_total[\"actual_participants\"].isna().sum())\n",
    "print(\"Decimales raros:\", (df_total[\"actual_participants\"].dropna() % 1 != 0).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_participants</th>\n",
       "      <th>source_file_year</th>\n",
       "      <th>academic_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4816106</th>\n",
       "      <td>4.7</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         actual_participants  source_file_year academic_year\n",
       "4816106                  4.7              2023       2023-24"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Inspección: si hay decimales, localizo las filas para ver de dónde vienen\n",
    "# (me interesa especialmente el año de origen del archivo y el academic_year)\n",
    "mask_dec = (df_total[\"actual_participants\"] % 1 != 0)\n",
    "df_total.loc[mask_dec, [\"actual_participants\", \"source_file_year\", \"academic_year\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Limpieza: convierto a nulo los valores imposibles y dejo la columna como entero nullable\n",
    "# - Si tiene decimales, lo considero inválido (no se puede tener 4.7 participantes)\n",
    "# - Si es 0, también lo considero inválido (en este dataset no tiene sentido 0 participantes)\n",
    "col = \"actual_participants\"\n",
    "\n",
    "mask_nonint = df_total[col].notna() & ((df_total[col] % 1) != 0)\n",
    "mask_zero = (df_total[col] == 0)\n",
    "\n",
    "df_total.loc[mask_nonint | mask_zero, col] = pd.NA\n",
    "df_total[col] = df_total[col].astype(\"Int64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<= 0 (conteo): 82785\n",
      "     participant_age academic_year  source_file_year\n",
      "61                -1       2013-14              2014\n",
      "126               -2       2013-14              2014\n",
      "162               -1       2013-14              2014\n",
      "175               -2       2013-14              2014\n",
      "183               -1       2014-15              2014\n",
      "237               -2       2013-14              2014\n",
      "560               -1       2013-14              2014\n",
      "663               -1       2013-14              2014\n",
      "821                0       2014-15              2014\n",
      "942               -1       2013-14              2014\n",
      "1011              -1       2013-14              2014\n",
      "1147               0       2013-14              2014\n",
      "1343              -1       2013-14              2014\n",
      "1602              -1       2013-14              2014\n",
      "1606               0       2013-14              2014\n",
      "1642              -1       2013-14              2014\n",
      "5851              -1       2013-14              2014\n",
      "5910              -1       2013-14              2014\n",
      "5941              -1       2013-14              2014\n",
      "6031              -1       2013-14              2014\n",
      "\n",
      ">= 65 (conteo): 16993\n",
      "      participant_age academic_year  source_file_year\n",
      "887                65       2014-15              2014\n",
      "5632               70       2013-14              2014\n",
      "6134               66       2013-14              2014\n",
      "6142               67       2013-14              2014\n",
      "6856              823       2014-15              2014\n",
      "8349               70       2014-15              2014\n",
      "9406              114       2014-15              2014\n",
      "11636              65       2013-14              2014\n",
      "12164             109       2013-14              2014\n",
      "12509             941       2014-15              2014\n",
      "12510             953       2014-15              2014\n",
      "13425             125       2014-15              2014\n",
      "13485              68       2014-15              2014\n",
      "13493              68       2014-15              2014\n",
      "14089             109       2014-15              2014\n",
      "14102              65       2014-15              2014\n",
      "14160            1922       2014-15              2014\n",
      "14161            1929       2014-15              2014\n",
      "14168            1923       2014-15              2014\n",
      "14169            1925       2014-15              2014\n",
      "\n",
      "Value counts (<=0):\n",
      "participant_age\n",
      "0      79845\n",
      "-1      2232\n",
      "-2       221\n",
      "-3        92\n",
      "-4        46\n",
      "-43       21\n",
      "-44       21\n",
      "-41       19\n",
      "-42       16\n",
      "-40       15\n",
      "-48       14\n",
      "-38       13\n",
      "-39       12\n",
      "-50       11\n",
      "-5        11\n",
      "-53       10\n",
      "-35        9\n",
      "-45        9\n",
      "-36        8\n",
      "-46        8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts (>=65):\n",
      "participant_age\n",
      "65      4081\n",
      "66      2836\n",
      "67      2078\n",
      "68      1595\n",
      "69      1193\n",
      "70       893\n",
      "71       713\n",
      "72       590\n",
      "73       432\n",
      "74       385\n",
      "75       282\n",
      "76       196\n",
      "77       156\n",
      "78       107\n",
      "79        68\n",
      "80        67\n",
      "81        47\n",
      "83        40\n",
      "82        37\n",
      "1821      35\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Antes de limpiar la edad, hago una revisión rápida de valores extremos.\n",
    "# La idea es convertir \"participant_age\" a numérico en una variable auxiliar (age_num)\n",
    "# para poder detectar edades imposibles (<= 0) o demasiado altas (>= 65).\n",
    "\n",
    "# 1) Convertir a numérico\n",
    "age_num = pd.to_numeric(\n",
    "    df_total[\"participant_age\"].replace({\"-\": pd.NA, \"\": pd.NA, \"0\": 0}),\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# 2) Revisar valores <= 0 (edades negativas o cero no tienen sentido)\n",
    "print(\"<= 0 (conteo):\", (age_num <= 0).sum())\n",
    "print(df_total.loc[age_num <= 0, [\"participant_age\", \"academic_year\", \"source_file_year\"]].head(20))\n",
    "\n",
    "# 3) Revisar valores >= 65 (en este análisis considero que son valores extremos / poco realistas)\n",
    "print(\"\\n>= 65 (conteo):\", (age_num >= 65).sum())\n",
    "print(df_total.loc[age_num >= 65, [\"participant_age\", \"academic_year\", \"source_file_year\"]].head(20))\n",
    "\n",
    "# 4) Distribución rápida de esos extremos para ver qué valores aparecen con más frecuencia\n",
    "print(\"\\nValue counts (<=0):\")\n",
    "print(df_total.loc[age_num <= 0, \"participant_age\"].value_counts(dropna=False).head(20))\n",
    "\n",
    "print(\"\\nValue counts (>=65):\")\n",
    "print(df_total.loc[age_num >= 65, \"participant_age\"].value_counts(dropna=False).head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN en participant_age: 621084\n",
      "participant_age\n",
      "21      680917\n",
      "<NA>    621084\n",
      "20      590851\n",
      "22      569740\n",
      "23      472454\n",
      "24      348161\n",
      "19      314029\n",
      "18      262116\n",
      "25      237262\n",
      "17      217568\n",
      "26      161223\n",
      "16      123771\n",
      "27      116001\n",
      "28       88994\n",
      "29       72125\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# Ahora sí hago la limpieza final de \"participant_age\".\n",
    "# 1) Convierto a numérico para que lo que no sea un número pase a NaN.\n",
    "# 2) Marco como nulos los valores fuera del rango que considero razonable para el análisis.\n",
    "# 3) Guardo el resultado como entero nullable (Int64), para permitir nulos sin convertir a float.\n",
    "\n",
    "age = pd.to_numeric(\n",
    "    df_total[\"participant_age\"].replace({\"-\": pd.NA, \"\": pd.NA}),\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Rango que considero válido: 10 a 65 años\n",
    "# Lo de fuera se deja como NA para no inventar datos\n",
    "age = age.mask((age < 10) | (age > 65))\n",
    "\n",
    "df_total[\"participant_age\"] = age.astype(\"Int64\")\n",
    "\n",
    "# Comprobación rápida: número de nulos y edades más frecuentes\n",
    "print(\"NaN en participant_age:\", df_total[\"participant_age\"].isna().sum())\n",
    "print(df_total[\"participant_age\"].value_counts(dropna=False).head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duración = 0 días: 16,069 (0.2703%) -> se convierte a NA\n",
      "Nulos tras limpieza: 16069\n",
      "Min/Mediana/Max: 1.0 29.0 1095.0\n"
     ]
    }
   ],
   "source": [
    "# Limpieza básica de \"mobility_duration\" (duración de la movilidad en días).\n",
    "# Aquí me centro en un error claro: duraciones = 0, que no tienen sentido para una movilidad real.\n",
    "# Primero cuantifico cuántos ceros hay, luego los convierto a nulo (NA) y saco un resumen rápido.\n",
    "\n",
    "col = \"mobility_duration\"\n",
    "\n",
    "# 1) Conteo de ceros (valores imposibles o errores de registro)\n",
    "n0 = (df_total[col] == 0).sum()\n",
    "print(f\"Duración = 0 días: {n0:,} ({(n0 / len(df_total)) * 100:.4f}%) -> se convierte a NA\")\n",
    "\n",
    "# 2) Reemplazo 0 por NA\n",
    "df_total.loc[df_total[col] == 0, col] = pd.NA\n",
    "\n",
    "# 3) Resumen rápido tras la limpieza\n",
    "print(\"Nulos tras limpieza:\", df_total[col].isna().sum())\n",
    "print(\"Min/Mediana/Max:\", df_total[col].min(), df_total[col].median(), df_total[col].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">365 días: 3190\n",
      ">730 días: 81\n"
     ]
    }
   ],
   "source": [
    "# Revisión adicional: duraciones muy largas.\n",
    "# No las elimino automáticamente, pero las cuantifico para saber si hay valores extremos\n",
    "# (por ejemplo, movilidades de más de un año o más de dos años).\n",
    "\n",
    "col = \"mobility_duration\"\n",
    "\n",
    "print(\">365 días:\", (df_total[col] > 365).sum())\n",
    "print(\">730 días:\", (df_total[col] > 730).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 columnas con más NA:\n",
      "project_reference         2481891\n",
      "education_level           2179182\n",
      "field_of_education        1993699\n",
      "participant_age            621084\n",
      "receiving_organization     567585\n",
      "sending_organization       323004\n",
      "fewer_opportunities        229867\n",
      "participant_country         35709\n",
      "participant_gender          33335\n",
      "mobility_duration           16069\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# En varias columnas de texto hay valores que en realidad significan \"desconocido\" o \"no aplica\",\n",
    "# pero vienen escritos como strings (por ejemplo \"unknown\", \"n/a\", \"???\", \"-\").\n",
    "# Para poder trabajar mejor con los datos, unifico todos esos casos y los convierto a NA.\n",
    "# Así, los análisis de nulos y las limpiezas posteriores son más consistentes.\n",
    "\n",
    "# Tokens típicos de \"nulo encubierto\" (no incluyo \"0\" por defecto porque en algunas columnas puede ser un valor válido)\n",
    "missing_tokens = {\n",
    "    \"unknown\", \"not specified\", \"none\", \"n/a\", \"na\",\n",
    "    \"?\", \"??\", \"???\", \"????\",\n",
    "    \"-\", \"--\", \"---\",\n",
    "    \"_\", \"__\",\n",
    "    \"? unknown ?\", \"??? - ? unknown ?\"\n",
    "}\n",
    "\n",
    "# Selecciono columnas de tipo texto (object o category)\n",
    "text_cols = df_total.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "\n",
    "# Reemplazo general en columnas de texto:\n",
    "# - quito espacios\n",
    "# - paso a minúsculas para comparar con los tokens\n",
    "# - convierto a NA cuando es cadena vacía o coincide con un token de missing\n",
    "for c in text_cols:\n",
    "    s = df_total[c].astype(\"string\").str.strip()\n",
    "    s_low = s.str.lower()\n",
    "    df_total[c] = s.mask((s == \"\") | (s_low.isin(missing_tokens)), pd.NA)\n",
    "\n",
    "# Caso específico: en \"receiving_city\" aparece el string \"0\" como valor inválido en algunos registros.\n",
    "# En este dataset lo trato como nulo para no confundirlo con una ciudad real.\n",
    "if \"receiving_city\" in df_total.columns:\n",
    "    df_total.loc[\n",
    "        df_total[\"receiving_city\"].astype(\"string\").str.strip() == \"0\",\n",
    "        \"receiving_city\"\n",
    "    ] = pd.NA\n",
    "\n",
    "# Resumen rápido: muestro las columnas con más valores nulos para tener una visión general\n",
    "na_counts = df_total.isna().sum().sort_values(ascending=False)\n",
    "print(\"Top 10 columnas con más NA:\")\n",
    "print(na_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "participant_profile\n",
      "Learner    4428605\n",
      "Staff      1500857\n",
      "<NA>          8841\n",
      "Other         7607\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# En \"participant_profile\" encuentro valores en distintos formatos (por ejemplo en mayúsculas).\n",
    "# Para evitar que \"LEARNERS\" y \"Learner\" se traten como categorías distintas, unifico etiquetas.\n",
    "# Después convierto la columna a tipo category para que quede más ordenada y ocupe menos memoria.\n",
    "\n",
    "df_total[\"participant_profile\"] = df_total[\"participant_profile\"].replace({\n",
    "    \"LEARNERS\": \"Learner\",\n",
    "    \"STAFF\": \"Staff\"\n",
    "})\n",
    "\n",
    "df_total[\"participant_profile\"] = df_total[\"participant_profile\"].astype(\"category\")\n",
    "\n",
    "# Comprobación rápida: veo cuántos registros hay de cada perfil (incluyendo nulos)\n",
    "print(df_total[\"participant_profile\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fewer_opportunities\n",
      "No      5001227\n",
      "Yes      714816\n",
      "<NA>     229867\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# La columna \"fewer_opportunities\" viene codificada como 0/1 (a veces como texto y a veces como número).\n",
    "# Para que sea más fácil de interpretar, la convierto a etiquetas \"No\" y \"Yes\".\n",
    "# Después la paso a tipo category, ya que solo tiene unas pocas categorías posibles.\n",
    "\n",
    "df_total[\"fewer_opportunities\"] = df_total[\"fewer_opportunities\"].replace({\n",
    "    \"0\": \"No\", 0: \"No\",\n",
    "    \"1\": \"Yes\", 1: \"Yes\"\n",
    "}).astype(\"category\")\n",
    "\n",
    "# Comprobación rápida: distribución de valores (incluyendo nulos)\n",
    "print(df_total[\"fewer_opportunities\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isced_level\n",
      "1          3996\n",
      "2         55627\n",
      "3        405814\n",
      "4        122182\n",
      "5        108643\n",
      "6       1908854\n",
      "7       1007928\n",
      "8        104122\n",
      "9         49562\n",
      "<NA>    2179182\n",
      "Name: count, dtype: Int64\n",
      "isced_group\n",
      "HE (6-8)              3020904\n",
      "<NA>                  2179182\n",
      "Pre-tertiary (1-5)     696262\n",
      "ISCED-9 / Other         49562\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# A partir de \"education_level\" quiero extraer el nivel ISCED (si aparece en el texto).\n",
    "# En el dataset suele venir en formatos como:\n",
    "# \"ISCED-6 - First cycle / Bachelor’s ...\" o similares.\n",
    "# La idea es quedarme solo con el número (1..9) para poder agrupar y filtrar mejor.\n",
    "\n",
    "# 1) Extraer el número de ISCED (si existe)\n",
    "df_total[\"isced_level\"] = (\n",
    "    df_total[\"education_level\"]\n",
    "      .astype(\"string\")                          # aseguro formato string para trabajar con .str\n",
    "      .str.extract(r\"ISCED-(\\d)\", expand=False)  # capturo el dígito después de \"ISCED-\"\n",
    "      .astype(\"Int64\")                           # lo dejo como entero nullable (permite NA)\n",
    ")\n",
    "\n",
    "# 2) Crear un grupo simplificado (más útil para el análisis del proyecto)\n",
    "# - HE (6-8): educación superior (Bachelor/Master/Doctorate)\n",
    "# - Pre-tertiary (1-5): niveles previos\n",
    "# - ISCED-9 / Other: otros casos\n",
    "# Si no se puede extraer ISCED, se queda como NA.\n",
    "df_total[\"isced_group\"] = pd.Series(pd.NA, index=df_total.index, dtype=\"string\")\n",
    "\n",
    "df_total.loc[df_total[\"isced_level\"].between(6, 8), \"isced_group\"] = \"HE (6-8)\"\n",
    "df_total.loc[df_total[\"isced_level\"].between(1, 5), \"isced_group\"] = \"Pre-tertiary (1-5)\"\n",
    "df_total.loc[df_total[\"isced_level\"] == 9, \"isced_group\"] = \"ISCED-9 / Other\"\n",
    "\n",
    "df_total[\"isced_group\"] = df_total[\"isced_group\"].astype(\"category\")\n",
    "\n",
    "# Comprobación rápida: distribución de niveles ISCED y de los grupos creados\n",
    "print(df_total[\"isced_level\"].value_counts(dropna=False).sort_index())\n",
    "print(df_total[\"isced_group\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activity_group\n",
      "HE                    2932465\n",
      "Staff/Training        1150150\n",
      "Youth/Volunteering     984875\n",
      "VET                    821099\n",
      "School                  51514\n",
      "Adult                    3468\n",
      "NaN                      2339\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# La columna \"activity_mob\" mezcla texto largo con un código al principio (en muchos casos).\n",
    "# Para analizarla mejor, separo:\n",
    "# 1) Un \"activity_code\" (si existe) extraído del inicio del texto.\n",
    "# 2) Un \"activity_group\" con categorías más generales (HE, VET, Youth, etc.).\n",
    "# Esto me permite filtrar y resumir sin depender de cientos de etiquetas distintas.\n",
    "\n",
    "# Limpio espacios para evitar problemas al extraer el código\n",
    "s = df_total[\"activity_mob\"].astype(\"string\").str.strip()\n",
    "\n",
    "# 1) Extraigo el código del formato \"CODIGO - descripción\"\n",
    "# Ejemplos típicos: \"HE-SMS - ...\", \"LM-VET - ...\", etc.\n",
    "code = s.str.extract(r\"^([A-Z]{1,4}(?:-[A-Z0-9]{1,10})+)\\s*-\\s*\", expand=False)\n",
    "df_total[\"activity_code\"] = code.astype(\"category\")\n",
    "\n",
    "\n",
    "def group_from_activity(text, code):\n",
    "    \"\"\"\n",
    "    Asigna una categoría general a cada registro de activity_mob.\n",
    "    Primero intento clasificar por el código (si está disponible).\n",
    "    Si no hay código o el formato es antiguo, intento clasificar por palabras clave del texto.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return pd.NA\n",
    "\n",
    "    t = str(text).lower()\n",
    "    c = str(code) if pd.notna(code) else \"\"\n",
    "\n",
    "    # 1) Clasificación por código (cuando existe)\n",
    "    if c:\n",
    "        if c.startswith(\"HE-\"):\n",
    "            return \"HE\"\n",
    "        if c.startswith(\"LM-\") and \"VET\" in c:\n",
    "            return \"VET\"\n",
    "        if c.startswith(\"LM-\") and (\"EXCH\" in c or \"YOU\" in c):\n",
    "            return \"Youth/Volunteering\"\n",
    "        if c.startswith(\"LM-\") and \"PUPIL\" in c:\n",
    "            return \"School\"\n",
    "        if c.startswith(\"LM-\") and \"ADULT\" in c:\n",
    "            return \"Adult\"\n",
    "        if c.startswith(\"SM-\") or c.startswith(\"OA-\"):\n",
    "            return \"Staff/Training\"\n",
    "\n",
    "    # 2) Clasificación por texto (para casos antiguos o sin código)\n",
    "    if (\n",
    "        \"student mobility for studies\" in t\n",
    "        or \"student mobility for traineeships\" in t\n",
    "        or \"higher education\" in t\n",
    "    ):\n",
    "        return \"HE\"\n",
    "\n",
    "    if (\n",
    "        \"youth exchanges\" in t\n",
    "        or \"mobility of youth workers\" in t\n",
    "        or \"european voluntary service\" in t\n",
    "        or \"volunteering\" in t\n",
    "    ):\n",
    "        return \"Youth/Volunteering\"\n",
    "\n",
    "    if (\n",
    "        \"vet learners\" in t\n",
    "        or \"mobility of vet learners\" in t\n",
    "        or \"erasmuspro\" in t\n",
    "        or \"vet\" in t\n",
    "    ):\n",
    "        return \"VET\"\n",
    "\n",
    "    if (\n",
    "        \"staff mobility\" in t\n",
    "        or \"staff training\" in t\n",
    "        or \"structured courses/training events\" in t\n",
    "        or \"training/teaching assignments abroad\" in t\n",
    "        or \"teaching/training assignments abroad\" in t\n",
    "        or \"job shadowing\" in t\n",
    "    ):\n",
    "        return \"Staff/Training\"\n",
    "\n",
    "    if \"school pupils\" in t or \"pupil\" in t:\n",
    "        return \"School\"\n",
    "\n",
    "    if \"adult learners\" in t:\n",
    "        return \"Adult\"\n",
    "\n",
    "    # Visitas/preparación/hosting (incluye Advance Planning Visit)\n",
    "    if (\n",
    "        \"advance planning visit\" in t\n",
    "        or \"preparatory visit\" in t\n",
    "        or \"invited experts\" in t\n",
    "        or \"hosting teachers\" in t\n",
    "    ):\n",
    "        return \"Staff/Training\"\n",
    "\n",
    "    # Si no encaja con nada, lo dejo como \"Other\"\n",
    "    return \"Other\"\n",
    "\n",
    "\n",
    "# Aplico la función para crear el grupo final.\n",
    "# Uso zip para pasar a la vez el texto y el código extraído.\n",
    "df_total[\"activity_group\"] = [\n",
    "    group_from_activity(txt, c)\n",
    "    for txt, c in zip(df_total[\"activity_mob\"], df_total[\"activity_code\"])\n",
    "]\n",
    "\n",
    "df_total[\"activity_group\"] = df_total[\"activity_group\"].astype(\"category\")\n",
    "\n",
    "# Comprobación rápida: distribución de grupos\n",
    "print(df_total[\"activity_group\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isced_macro\n",
      "NaN                                                     1993699\n",
      "04 - Business, administration and law                    814678\n",
      "02 - Arts and humanities                                 708864\n",
      "07 - Engineering, manufacturing and construction         559492\n",
      "03 - Social sciences, journalism and information         411092\n",
      "01 - Education                                           405750\n",
      "09 - Health and welfare                                  311763\n",
      "10 - Services                                            266341\n",
      "05 - Natural sciences, mathematics and statistics        223087\n",
      "06 - Information and Communication Technologies          137752\n",
      "08 - Agriculture, forestry, fisheries and veterinary     108669\n",
      "99 - Not classified                                        4717\n",
      "00 - Generic programmes and qualifications                    6\n",
      "Name: count, dtype: int64\n",
      "No clasificados (99): 4717\n"
     ]
    }
   ],
   "source": [
    "# Este bloque sirve para agrupar \"field_of_education\" en categorías amplias (ISCED-F broad fields).\n",
    "# En el dataset, esta columna puede venir con:\n",
    "# - un código al inicio (por ejemplo \"0410 - Business and administration\")\n",
    "# - o directamente texto (a veces sin código)\n",
    "# Lo que hago es:\n",
    "# 1) Intentar extraer un código si existe al principio (4 dígitos / 2 dígitos / 1 dígito)\n",
    "# 2) Si no hay código, aplicar reglas por palabras clave para asignar un macro-campo\n",
    "# 3) Si no encaja con nada, lo marco como \"99 - Not classified\"\n",
    "#\n",
    "# Nota: muchos registros pueden venir con NA en field_of_education, por eso aparece un número alto de NaN.\n",
    "\n",
    "COL = \"field_of_education\"  # ajusta si tu columna tiene espacios\n",
    "\n",
    "# Etiquetas amplias (2 dígitos) para ISCED broad fields\n",
    "broad_labels = {\n",
    "    \"00\": \"00 - Generic programmes and qualifications\",\n",
    "    \"01\": \"01 - Education\",\n",
    "    \"02\": \"02 - Arts and humanities\",\n",
    "    \"03\": \"03 - Social sciences, journalism and information\",\n",
    "    \"04\": \"04 - Business, administration and law\",\n",
    "    \"05\": \"05 - Natural sciences, mathematics and statistics\",\n",
    "    \"06\": \"06 - Information and Communication Technologies\",\n",
    "    \"07\": \"07 - Engineering, manufacturing and construction\",\n",
    "    \"08\": \"08 - Agriculture, forestry, fisheries and veterinary\",\n",
    "    \"09\": \"09 - Health and welfare\",\n",
    "    \"10\": \"10 - Services\",\n",
    "    \"99\": \"99 - Not classified\",\n",
    "}\n",
    "\n",
    "# Reglas por texto: si la descripción contiene ciertas palabras clave, asigno un macro-código\n",
    "rules = [\n",
    "    (r\"\\beducation\\b|teacher training|pre-?school\", \"01\"),\n",
    "    (r\"arts?|fine arts|handicrafts|music|performing|audio-visual|fashion|design|humanities|\"\n",
    "     r\"history|archaeology|philosophy|ethics|religion|theology|languages?|linguistics|literature\", \"02\"),\n",
    "    (r\"social and behavioural|economics|political|psychology|sociology|cultural studies|\"\n",
    "     r\"journalism|reporting|library|archival\", \"03\"),\n",
    "    (r\"accounting|taxation|finance|banking|insurance|management|administration|marketing|advertising|\"\n",
    "     r\"secretarial|office work|wholesale|retail|business\\b|law\\b|legal|juris\", \"04\"),\n",
    "    (r\"biolog|biochem|chemistry|earth sciences|physics|physical sciences|mathematics|statistics|\"\n",
    "     r\"natural sciences|environment\", \"05\"),\n",
    "    (r\"\\bict\\b|information and communication technologies|computer use|database|network|software|applications\", \"06\"),\n",
    "    (r\"engineering|electricity|energy\\b|electronics|automation|mechanics|metal trades|motor vehicles|ships|aircraft|\"\n",
    "     r\"manufacturing|processing|materials|textiles|mining|extraction|architecture|building|civil engineering|construction\", \"07\"),\n",
    "    (r\"agricult|crop|livestock|horticult|forestry|fisheries|veterinar\", \"08\"),\n",
    "    (r\"\\bhealth\\b|medicine|medical|nursing|midwifery|therapy|rehabilitation|pharmacy|dental|\"\n",
    "     r\"welfare|care of the elderly|disabled adults|child care|youth services|social work|counselling\", \"09\"),\n",
    "    (r\"personal services|domestic services|hair|beauty|hotel|restaurants|catering|\"\n",
    "     r\"sports|travel|tourism|leisure|hygiene|occupational health and safety|community sanitation|\"\n",
    "     r\"security services|military|defence|transport|\\bservices\\b\", \"10\"),\n",
    "    (r\"generic programmes|eqf-\\d\", \"00\"),\n",
    "    (r\"^not-?classified\\b\", \"99\"),\n",
    "]\n",
    "\n",
    "def to_macro_code(val):\n",
    "    # Si el valor es nulo, lo mantengo como NA\n",
    "    if pd.isna(val):\n",
    "        return pd.NA\n",
    "\n",
    "    s = str(val).strip().lower()\n",
    "\n",
    "    # 1) Si empieza con código, lo intento convertir a un macro-código de 2 dígitos\n",
    "    # Acepto \"#### - ...\", \"## - ...\" o \"# - ...\"\n",
    "    m = re.match(r\"^(\\d{4}|\\d{2}|\\d)\\s*-\\s*\", s)\n",
    "    if m:\n",
    "        raw = m.group(1)\n",
    "        if len(raw) == 4:\n",
    "            code2 = raw[:2]\n",
    "        elif len(raw) == 2:\n",
    "            code2 = raw\n",
    "        else:\n",
    "            code2 = f\"0{raw}\"\n",
    "\n",
    "        if code2 in broad_labels:\n",
    "            return code2\n",
    "\n",
    "    # 2) Si no hay código, aplico reglas de palabras clave\n",
    "    for pat, code in rules:\n",
    "        if re.search(pat, s):\n",
    "            return code\n",
    "\n",
    "    # 3) Si no encaja, lo marco como no clasificado\n",
    "    return \"99\"\n",
    "\n",
    "\n",
    "# Aplico la función para generar el macro-código y su etiqueta\n",
    "df_total[\"isced_macro_code\"] = df_total[COL].apply(to_macro_code)\n",
    "df_total[\"isced_macro\"] = df_total[\"isced_macro_code\"].map(broad_labels).astype(\"category\")\n",
    "\n",
    "# Resumen rápido para ver si la clasificación tiene sentido\n",
    "print(df_total[\"isced_macro\"].value_counts(dropna=False))\n",
    "print(\"No clasificados (99):\", (df_total[\"isced_macro_code\"] == \"99\").sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "participant_country únicos: 255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alizd\\AppData\\Local\\Temp\\ipykernel_4364\\3878975602.py:132: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  multi = tmp.groupby(base + \"_code\")[base + \"_name\"].nunique()\n",
      "C:\\Users\\alizd\\AppData\\Local\\Temp\\ipykernel_4364\\3878975602.py:132: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  multi = tmp.groupby(base + \"_code\")[base + \"_name\"].nunique()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL sending_country códigos con >1 nombre: 0\n",
      "FINAL receiving_country códigos con >1 nombre: 0\n"
     ]
    }
   ],
   "source": [
    "# En el dataset hay tres columnas relacionadas con países:\n",
    "# - participant_country: suele venir solo como nombre (sin código delante).\n",
    "# - sending_country / receiving_country: suelen venir como \"XX - Country name\" (XX = ISO2).\n",
    "#\n",
    "# Problema:\n",
    "# - El mismo país puede aparecer con nombres distintos (por ejemplo \"Türkiye\" vs \"Turkey\"),\n",
    "#   o con variantes largas (\"Iran (Islamic Republic of)\").\n",
    "# - En sending/receiving, para un mismo código ISO2 puede haber pequeñas diferencias en el nombre\n",
    "#   (espacios, capitalización, variantes), lo que rompe conteos si no se unifica.\n",
    "#\n",
    "# Estrategia:\n",
    "# 1) participant_country: limpio espacios y aplico un diccionario de equivalencias (name_map).\n",
    "# 2) sending_country y receiving_country: normalizo por código ISO2:\n",
    "#    - extraigo el código y el nombre,\n",
    "#    - elijo el nombre más frecuente como “canónico” por código,\n",
    "#    - creo columnas auxiliares <col>_code y <col>_name,\n",
    "#    - reconstruyo la columna original en formato \"XX - NombreCanónico\".\n",
    "# 3) Estándar global: fuerzo que sending y receiving usen el mismo nombre canónico por código,\n",
    "#    para que un código no tenga un nombre distinto según la columna.\n",
    "# 4) Check final: verifico que no quedan códigos con más de un nombre.\n",
    "\n",
    "\n",
    "\n",
    "# 1) Normalización de participant_country (solo nombre, sin ISO2)\n",
    "name_map = {\n",
    "    \"Türkiye\": \"Turkey\",\n",
    "    \"Czechia\": \"Czech Republic\",\n",
    "    \"Russian Federation\": \"Russia\",\n",
    "    \"Moldova (Republic of)\": \"Moldova\",\n",
    "    \"Republic of Moldova\": \"Moldova\",\n",
    "    \"China (People's Republic of)\": \"China\",\n",
    "    \"Iran (Islamic Republic of)\": \"Iran\",\n",
    "    \"Syrian Arab Republic\": \"Syria\",\n",
    "    \"Viet Nam\": \"Vietnam\",\n",
    "    \"Tanzania (United Republic of)\": \"Tanzania\",\n",
    "    \"Korea (Republic of)\": \"South Korea\",\n",
    "    \"Korea (Democratic People's Republic of)\": \"North Korea\",\n",
    "    \"Cabo Verde\": \"Cape Verde\",\n",
    "    \"Åland islands\": \"Aland Islands\",\n",
    "    \"United States of America\": \"United States\",\n",
    "    \"United States Minor outlying islands\": \"United States Minor Outlying Islands\",\n",
    "    \"St Lucia\": \"Saint Lucia\",\n",
    "    \"St Kitts and Nevis\": \"Saint Kitts and Nevis\",\n",
    "    \"St Vincent and the Grenadines\": \"Saint Vincent and the Grenadines\",\n",
    "    \"Isle Of Man\": \"Isle of Man\",\n",
    "    \"The Republic of North Macedonia\": \"North Macedonia\",\n",
    "    \"Kosovo * UN resolution\": \"Kosovo\",\n",
    "    \"Congo (Democratic Republic of)\": \"Democratic Republic of the Congo\",\n",
    "    \"Lao (People's Democratic Republic)\": \"Laos\",\n",
    "    \"Lao People's Democratic Republic\": \"Laos\",\n",
    "    \"Brunei Darussalam\": \"Brunei\",\n",
    "    \"French Southern and Antarctic Territories\": \"French Southern Territories\",\n",
    "}\n",
    "\n",
    "df_total[\"participant_country\"] = (\n",
    "    df_total[\"participant_country\"]\n",
    "      .astype(\"string\")\n",
    "      .str.strip()\n",
    "      .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "      .replace(name_map)\n",
    "      .astype(\"category\")\n",
    ")\n",
    "\n",
    "print(\"participant_country únicos:\", df_total[\"participant_country\"].nunique(dropna=True))\n",
    "\n",
    "\n",
    "# 2) Normalización por ISO2 para sending_country y receiving_country\n",
    "def normalize_country_by_code(df, col):\n",
    "    \"\"\"\n",
    "    Normaliza una columna que viene como 'XX - Name' (XX = ISO2).\n",
    "    Para cada código, se usa el nombre más frecuente como nombre canónico.\n",
    "    \"\"\"\n",
    "    s = df[col].astype(\"string\").str.strip()\n",
    "\n",
    "    # Validación rápida: compruebo que existe el patrón \"XX - ...\"\n",
    "    has_code = s.str.match(r\"^[A-Z]{2}\\s*[-–]\\s*\", na=False).any()\n",
    "    if not has_code:\n",
    "        raise ValueError(f\"{col} no parece venir como 'XX - Name'. No puedo extraer ISO2.\")\n",
    "\n",
    "    # Extraigo código y nombre\n",
    "    code = s.str.extract(r\"^([A-Z]{2})\\s*[-–]\\s*\", expand=False)\n",
    "    name = s.str.replace(r\"^[A-Z]{2}\\s*[-–]\\s*\", \"\", regex=True).str.strip()\n",
    "    name = name.str.replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "    # Nombre canónico por código: el más frecuente\n",
    "    canon_name = (\n",
    "        pd.DataFrame({\"code\": code, \"name\": name})\n",
    "          .dropna()\n",
    "          .groupby(\"code\")[\"name\"]\n",
    "          .agg(lambda x: x.value_counts().idxmax())\n",
    "    )\n",
    "\n",
    "    # Guardo columnas auxiliares y reconstruyo la columna original normalizada\n",
    "    df[col + \"_code\"] = code.astype(\"category\")\n",
    "    df[col + \"_name\"] = code.map(canon_name).astype(\"category\")\n",
    "    df[col] = (\n",
    "        df[col + \"_code\"].astype(\"string\") + \" - \" + df[col + \"_name\"].astype(\"string\")\n",
    "    ).astype(\"category\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_total = normalize_country_by_code(df_total, \"sending_country\")\n",
    "df_total = normalize_country_by_code(df_total, \"receiving_country\")\n",
    "\n",
    "\n",
    "# 3) Estándar global: mismo nombre por código en sending y receiving\n",
    "both = pd.concat([\n",
    "    df_total[[\"sending_country_code\", \"sending_country_name\"]]\n",
    "      .rename(columns={\"sending_country_code\": \"code\", \"sending_country_name\": \"name\"}),\n",
    "    df_total[[\"receiving_country_code\", \"receiving_country_name\"]]\n",
    "      .rename(columns={\"receiving_country_code\": \"code\", \"receiving_country_name\": \"name\"}),\n",
    "], ignore_index=True).dropna()\n",
    "\n",
    "global_name = both.groupby(\"code\")[\"name\"].agg(lambda x: x.value_counts().idxmax())\n",
    "\n",
    "df_total[\"sending_country_name\"] = df_total[\"sending_country_code\"].map(global_name).astype(\"category\")\n",
    "df_total[\"receiving_country_name\"] = df_total[\"receiving_country_code\"].map(global_name).astype(\"category\")\n",
    "\n",
    "df_total[\"sending_country\"] = (\n",
    "    df_total[\"sending_country_code\"].astype(\"string\") + \" - \" + df_total[\"sending_country_name\"].astype(\"string\")\n",
    ").astype(\"category\")\n",
    "\n",
    "df_total[\"receiving_country\"] = (\n",
    "    df_total[\"receiving_country_code\"].astype(\"string\") + \" - \" + df_total[\"receiving_country_name\"].astype(\"string\")\n",
    ").astype(\"category\")\n",
    "\n",
    "\n",
    "# 4) Check final: confirmo que no hay códigos con más de un nombre canónico\n",
    "for base in [\"sending_country\", \"receiving_country\"]:\n",
    "    tmp = df_total[[base + \"_code\", base + \"_name\"]].dropna()\n",
    "    multi = tmp.groupby(base + \"_code\")[base + \"_name\"].nunique()\n",
    "    print(\"FINAL\", base, \"códigos con >1 nombre:\", int((multi > 1).sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "receiving_city\n",
      "Únicos: 94344\n",
      "Filas con mojibake (�): 84140\n",
      "Ejemplos con mojibake:\n",
      "24           Cefal�\n",
      "69     St. Julian�S\n",
      "70     St. Julian�S\n",
      "71     St. Julian�S\n",
      "90    San Sebasti�N\n",
      "Name: receiving_city, dtype: category\n",
      "Categories (94344, object): ['', ' Berne', ' Pons', ' Puigdalber Barcelona', ..., '�������', '������� ����', '��������', '���������']\n",
      "\n",
      "sending_city\n",
      "Únicos: 46021\n",
      "Filas con mojibake (�): 89959\n",
      "Ejemplos con mojibake:\n",
      "12        G�Nserndorf\n",
      "16    Gro�-Enzersdorf\n",
      "17    Gro�-Enzersdorf\n",
      "20         St. P�Lten\n",
      "21         St. P�Lten\n",
      "Name: sending_city, dtype: category\n",
      "Categories (46021, object): ['', ' Paris', '\"59155 Faches Thumesnil\"', '\"59390 Lys Lez Lannoy\"', ..., '���������', '���������(Botevgrad)', '����������', '�����������']\n"
     ]
    }
   ],
   "source": [
    "# - Reduzco variaciones de escritura en nombres de ciudades (espacios extra, sufijos tipo CEDEX,\n",
    "#   puntuación, distritos como \"Paris 16\", tildes, etc.).\n",
    "# - Unifico algunas variantes/exónimos frecuentes a una forma estándar en inglés.\n",
    "# Nota sobre codificación:\n",
    "# - En algunos registros puede aparecer el carácter \"�\". Esto suele indicar un problema\n",
    "#   de encoding ya presente en los datos originales. Aquí no se corrige pero se cuantifica como chequeo de calidad.\n",
    "\n",
    "def strip_accents(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Quita tildes/diacríticos para reducir variantes del mismo nombre.\n",
    "    Ejemplo: \"Málaga\" -> \"Malaga\".\n",
    "    \"\"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    return s.replace(\"ß\", \"ss\")\n",
    "\n",
    "\n",
    "# Mapa de exónimos/variantes frecuentes a una forma estándar en inglés\n",
    "english_map = {\n",
    "    \"Wien\": \"Vienna\",\n",
    "    \"Praha\": \"Prague\",\n",
    "    \"Bruxelles\": \"Brussels\",\n",
    "    \"Roma\": \"Rome\",\n",
    "    \"Milano\": \"Milan\",\n",
    "    \"Lisboa\": \"Lisbon\",\n",
    "    \"Warszawa\": \"Warsaw\",\n",
    "    \"Oporto\": \"Porto\",\n",
    "    \"Bologne\": \"Bologna\",\n",
    "    \"Bolonia\": \"Bologna\",\n",
    "    \"Firenze\": \"Florence\",\n",
    "    \"Torino\": \"Turin\",\n",
    "    \"Kobenhavn\": \"Copenhagen\",\n",
    "    \"Sevilla\": \"Seville\",\n",
    "}\n",
    "\n",
    "\n",
    "def clean_city_series(series: pd.Series, english_map: dict) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Limpieza básica de nombres de ciudades.\n",
    "    Aplico reglas simples para homogenizar el texto sin normalizar ciudad por ciudad.\n",
    "    \"\"\"\n",
    "    s = series.astype(\"string\").str.strip()\n",
    "\n",
    "    # 1) Normalizar espacios\n",
    "    s = s.str.replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "    # 2) Quitar sufijos tipo \"CEDEX\" (comunes en direcciones de Francia)\n",
    "    s = s.str.replace(r\"\\s+cedex\\s*\\d*\\s*$\", \"\", regex=True, case=False)\n",
    "\n",
    "    # 3) Quitar puntuación final típica\n",
    "    s = s.str.replace(r\"[.,;:?]+$\", \"\", regex=True)\n",
    "\n",
    "    # 4) Quitar números de distrito al final (Paris 16, Dublin 2, etc.)\n",
    "    s = s.str.replace(r\"\\s+\\d{1,3}\\s*$\", \"\", regex=True)\n",
    "\n",
    "    # 5) Quitar tildes/diacríticos y estandarizar capitalización\n",
    "    s = s.apply(lambda x: strip_accents(str(x)) if pd.notna(x) else pd.NA)\n",
    "    s = s.str.title()\n",
    "\n",
    "    # 6) Unificar exónimos/variantes frecuentes\n",
    "    s = s.replace(english_map)\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "# Aplico la limpieza a ambas columnas de ciudad si existen en el DataFrame\n",
    "city_cols = [\"receiving_city\", \"sending_city\"]\n",
    "\n",
    "for col in city_cols:\n",
    "    if col in df_total.columns:\n",
    "        df_total[col] = clean_city_series(df_total[col], english_map).astype(\"category\")\n",
    "\n",
    "        # Chequeo ligero: número de únicos y presencia de mojibake (�)\n",
    "        print(f\"\\n{col}\")\n",
    "        print(\"Únicos:\", df_total[col].nunique(dropna=True))\n",
    "\n",
    "        bad = df_total[col].astype(\"string\").str.contains(\"�\", na=False).sum()\n",
    "        print(\"Filas con mojibake (�):\", bad)\n",
    "\n",
    "        # Ejemplos mínimos (solo si existe mojibake) para poder inspeccionarlo sin generar mucho output\n",
    "        if bad > 0:\n",
    "            print(\"Ejemplos con mojibake:\")\n",
    "            print(df_total.loc[df_total[col].astype(\"string\").str.contains(\"�\", na=False), col].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "academic_year\n",
      "2017-18    673550\n",
      "2018-19    727899\n",
      "2019-20    480060\n",
      "2020-21     93710\n",
      "2021-22    351358\n",
      "2022-23    580488\n",
      "2023-24    950429\n",
      "2024-25    323383\n",
      "Name: count, dtype: Int64\n",
      "Filas: 4180877\n"
     ]
    }
   ],
   "source": [
    "# Voy a preparar un dataset filtrado para el análisis (el que luego exportaré para Power BI).\n",
    "# Pasos:\n",
    "# 1) Creo year_start a partir de academic_year (por ejemplo \"2019-20\" -> 2019).\n",
    "# 2) Filtro el periodo 2017–2024 por año de inicio.\n",
    "# 3) Comparo criterios para definir HE (ISCED, activity_group, field) como evidencia.\n",
    "# 4) Defino el dataset final HE por ISCED 6–8 y añado una bandera he_strict.\n",
    "\n",
    "df = df_total.copy()\n",
    "\n",
    "df[\"year_start\"] = pd.to_numeric(\n",
    "    df[\"academic_year\"].astype(\"string\").str.extract(r\"(\\d{4})\")[0],\n",
    "    errors=\"coerce\"\n",
    ").astype(\"Int64\")\n",
    "\n",
    "df_17_24 = df[df[\"year_start\"].between(2017, 2024)].copy()\n",
    "\n",
    "print(df_17_24[\"academic_year\"].value_counts().sort_index().tail(10))\n",
    "print(\"Filas:\", len(df_17_24))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Filtro    Filas  % sobre df_17_24\n",
      "0                      ISCED 6-8 (criterio base)  2082071             49.80\n",
      "1                           activity_group == HE  2058366             49.23\n",
      "2                      field == Higher Education  2312575             55.31\n",
      "3  ISCED 6-8 AND activity_group HE (HE estricto)  1887691             45.15\n",
      "4                         ISCED 6-8 AND field HE  2014128             48.17\n",
      "5                 field HE AND activity_group HE  2058366             49.23\n",
      "6   ISCED 6-8 AND field HE AND activity_group HE  1887691             45.15\n"
     ]
    }
   ],
   "source": [
    "#Comparación de criterios para definir HE (evidencia)\n",
    "mask_isced = df_17_24[\"isced_level\"].isin([6, 7, 8])\n",
    "mask_act   = df_17_24[\"activity_group\"].eq(\"HE\")\n",
    "mask_field = df_17_24[\"field\"].eq(\"Higher Education\")\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Filtro\": [\n",
    "        \"ISCED 6-8 (criterio base)\",\n",
    "        \"activity_group == HE\",\n",
    "        \"field == Higher Education\",\n",
    "        \"ISCED 6-8 AND activity_group HE (HE estricto)\",\n",
    "        \"ISCED 6-8 AND field HE\",\n",
    "        \"field HE AND activity_group HE\",\n",
    "        \"ISCED 6-8 AND field HE AND activity_group HE\",\n",
    "    ],\n",
    "    \"Filas\": [\n",
    "        mask_isced.sum(),\n",
    "        mask_act.sum(),\n",
    "        mask_field.sum(),\n",
    "        (mask_isced & mask_act).sum(),\n",
    "        (mask_isced & mask_field).sum(),\n",
    "        (mask_field & mask_act).sum(),\n",
    "        (mask_isced & mask_field & mask_act).sum(),\n",
    "    ]\n",
    "})\n",
    "\n",
    "summary[\"% sobre df_17_24\"] = (summary[\"Filas\"] / len(df_17_24) * 100).round(2)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HE (ISCED 6-8): 2082071\n",
      "HE estricto (dentro de HE): 1887691\n",
      "academic_year\n",
      "2017-18    358415\n",
      "2018-19    373111\n",
      "2019-20    308735\n",
      "2020-21     81643\n",
      "2021-22    135559\n",
      "2022-23    221953\n",
      "2023-24    396792\n",
      "2024-25    205863\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "#Dataset final HE para Power BI (HE por ISCED 6–8 + bandera he_strict)\n",
    "df_17_24[\"he_isced\"] = mask_isced\n",
    "df_17_24[\"he_strict\"] = mask_isced & (df_17_24[\"activity_group\"] == \"HE\")\n",
    "\n",
    "df_he = df_17_24[df_17_24[\"he_isced\"]].copy()\n",
    "\n",
    "print(\"HE (ISCED 6-8):\", len(df_he))\n",
    "print(\"HE estricto (dentro de HE):\", df_he[\"he_strict\"].sum())\n",
    "print(df_he[\"academic_year\"].value_counts().sort_index().tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2082071 entries, 1432186 to 5944929\n",
      "Data columns (total 37 columns):\n",
      " #   Column                    Dtype         \n",
      "---  ------                    -----         \n",
      " 0   academic_year             string        \n",
      " 1   activity_mob              string        \n",
      " 2   actual_participants       Int64         \n",
      " 3   education_level           string        \n",
      " 4   fewer_opportunities       category      \n",
      " 5   field                     string        \n",
      " 6   field_of_education        string        \n",
      " 7   mobility_duration         float64       \n",
      " 8   mobility_start_month      string        \n",
      " 9   participant_age           Int64         \n",
      " 10  participant_country       category      \n",
      " 11  participant_gender        string        \n",
      " 12  participant_profile       category      \n",
      " 13  project_reference         string        \n",
      " 14  receiving_city            category      \n",
      " 15  receiving_country         category      \n",
      " 16  receiving_organization    string        \n",
      " 17  sending_city              category      \n",
      " 18  sending_country           category      \n",
      " 19  sending_organization      string        \n",
      " 20  source_file_year          int64         \n",
      " 21  mobility_start_ym         datetime64[ns]\n",
      " 22  mobility_start_year       Int64         \n",
      " 23  mobility_start_month_num  Int64         \n",
      " 24  isced_level               Int64         \n",
      " 25  isced_group               category      \n",
      " 26  activity_code             category      \n",
      " 27  activity_group            category      \n",
      " 28  isced_macro_code          object        \n",
      " 29  isced_macro               category      \n",
      " 30  sending_country_code      category      \n",
      " 31  sending_country_name      category      \n",
      " 32  receiving_country_code    category      \n",
      " 33  receiving_country_name    category      \n",
      " 34  year_start                Int64         \n",
      " 35  he_isced                  boolean       \n",
      " 36  he_strict                 boolean       \n",
      "dtypes: Int64(6), boolean(2), category(15), datetime64[ns](1), float64(1), int64(1), object(1), string(10)\n",
      "memory usage: 413.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_he.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset final para Power BI: me quedo solo con las columnas que voy a usar en el análisis.\n",
    "# Para países, conservo ISO2 (code) + nombre (name) porque es más robusto para mapas y para un modelo estrella.\n",
    "\n",
    "cols_keep = [\n",
    "    \"academic_year\", \"year_start\",\n",
    "    \"mobility_start_ym\", \"mobility_start_year\", \"mobility_start_month_num\",\n",
    "\n",
    "    # Países (ISO2 + nombre)\n",
    "    \"sending_country_code\", \"sending_country_name\",\n",
    "    \"receiving_country_code\", \"receiving_country_name\",\n",
    "\n",
    "    # participant_country viene como nombre (no ISO2 en este pipeline)\n",
    "    \"participant_country\",\n",
    "\n",
    "    # Variables demográficas / flags\n",
    "    \"participant_gender\", \"participant_profile\", \"fewer_opportunities\",\n",
    "\n",
    "    # Métricas numéricas\n",
    "    \"participant_age\", \"mobility_duration\", \"actual_participants\",\n",
    "\n",
    "    # Educación y actividad\n",
    "    \"isced_level\", \"isced_group\",\n",
    "    \"activity_group\",\n",
    "    \"isced_macro\",\n",
    "\n",
    "    # Flag para análisis conservador dentro de HE\n",
    "    \"he_strict\",\n",
    "]\n",
    "\n",
    "df_he_pbi = df_he[cols_keep].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Exporto el dataset final en dos formatos:\n# - Parquet (recomendado): comprimido, rápido de cargar, conserva tipos de datos.\n# - CSV (universal): para compatibilidad con herramientas que no soporten Parquet.\n# La carpeta de salida se crea si no existe.\n\nimport os\n\noutput_dir = \"data/processed\"\nos.makedirs(output_dir, exist_ok=True)\n\n# 1) Exporto a Parquet (preferido para Power BI)\nparquet_path = os.path.join(output_dir, \"erasmus_he_2017_2024.parquet\")\ndf_he_pbi.to_parquet(parquet_path, index=False)\n\n# 2) Exporto a CSV (fallback)\ncsv_path = os.path.join(output_dir, \"erasmus_he_2017_2024.csv\")\ndf_he_pbi.to_csv(csv_path, index=False)\n\n# Comprobación rápida: confirmo la exportación\nprint(f\"Exportados {len(df_he_pbi):,} registros en {output_dir}/\")\nprint(f\"  - erasmus_he_2017_2024.parquet ({os.path.getsize(parquet_path) / 1e6:.1f} MB)\")\nprint(f\"  - erasmus_he_2017_2024.csv ({os.path.getsize(csv_path) / 1e6:.1f} MB)\")"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}